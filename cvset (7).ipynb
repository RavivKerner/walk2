{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# import and download","metadata":{}},{"cell_type":"code","source":"import gc\nfrom pathlib import Path\nimport cudf\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm \n# from sklearn.preprocessing import  MinMaxScaler\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-05-25T09:00:23.286450Z","iopub.execute_input":"2023-05-25T09:00:23.286905Z","iopub.status.idle":"2023-05-25T09:00:26.998865Z","shell.execute_reply.started":"2023-05-25T09:00:23.286782Z","shell.execute_reply":"2023-05-25T09:00:26.998080Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\ndef f():\n    for i in range(10):\n        print(i)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T03:28:26.913594Z","iopub.execute_input":"2023-05-12T03:28:26.913934Z","iopub.status.idle":"2023-05-12T03:28:26.926867Z","shell.execute_reply.started":"2023-05-12T03:28:26.913875Z","shell.execute_reply":"2023-05-12T03:28:26.926058Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%debug\nf()","metadata":{"execution":{"iopub.status.busy":"2023-05-12T03:28:36.190022Z","iopub.execute_input":"2023-05-12T03:28:36.190473Z","iopub.status.idle":"2023-05-12T05:16:32.211565Z","shell.execute_reply.started":"2023-05-12T03:28:36.190439Z","shell.execute_reply":"2023-05-12T05:16:32.210773Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"NOTE: Enter 'c' at the ipdb>  prompt to continue execution.\n> \u001b[0;32m<string>\u001b[0m(2)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  s\n"},{"name":"stdout","text":"--Call--\n> \u001b[0;32m/tmp/ipykernel_17/3846476012.py\u001b[0m(1)\u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_17/3846476012.py\u001b[0m(2)\u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m----> 2 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  p i\n"},{"name":"stdout","text":"0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_17/3846476012.py\u001b[0m(3)\u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  p i\n"},{"name":"stdout","text":"0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"0\n> \u001b[0;32m/tmp/ipykernel_17/3846476012.py\u001b[0m(2)\u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m----> 2 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"> \u001b[0;32m/tmp/ipykernel_17/3846476012.py\u001b[0m(3)\u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  n\n"},{"name":"stdout","text":"1\n> \u001b[0;32m/tmp/ipykernel_17/3846476012.py\u001b[0m(2)\u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m----> 2 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  p i\n"},{"name":"stdout","text":"1\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  q\n"}]},{"cell_type":"code","source":"continue to clean notebook , all remarks and the recall generatoe loops\nClean rules\ntrain on valid week +1 like the last cadidtes is 0 and no -1 ts2 = TopSales2(trans, n_weeks=10, n_articles=10, valid_week=valid_week+1)\ncheck age binning running well (its category) maybe can run in cudf if change to int\ncandidates['customer_id'] = candidates['customer_id'].astype('int32') -added afetr cusid mapping ---added cusid customer_id still 64\n\nremove columns with low feature importance\ndown sampling negative sample (i.e neg = 30*pos, or winner sulotion post 1st place)\nreplace description with word2vec embadding\n    \n# avoid chnging to pandas as much as possible, check if .values can solve like in this case:\npreds = preds.to_pandas() #check if cuold avoid ad test in CV probably can avoid \nvc = trans.article_id.value_counts()\nfor i in vc.index.values[1000:1010]: #with no .values we get error to change to pandas\n    print(i)\n \nFurthur study:\nvalidation test model analysis (trian vs ble overfit bla)  - output the score interations - shuold have it in code somewhere hidden\n\nplot the desicon tree\n\n#-------trrials to save the model:\n# import lightgbm\n# ranker.booster_.save_model('mode.txt')\nor\n# ranker.save_model('mode.txt')\n# bst = lightgbm.Booster(model_file='./mode.txt')\nor \nfrom sklearn.externals import joblib\n# save model\njoblib.dump(lgbmodel, 'lgb.pkl')\n# load model\ngbm_pickle = joblib.load('lgb.pkl')\n#-------trrials to save the model:\n","metadata":{"execution":{"iopub.status.busy":"2023-03-24T07:03:39.415336Z","iopub.execute_input":"2023-03-24T07:03:39.415604Z","iopub.status.idle":"2023-03-24T07:03:39.428218Z","shell.execute_reply.started":"2023-03-24T07:03:39.415575Z","shell.execute_reply":"2023-03-24T07:03:39.427054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Future todo to improve score:\n    1 optimize model with parameter tuning\n    2 make addtional model and shuffle\n    3 add features to customer, articles:\n        Add feature mean channel for customer and for article\n        start to work on #department_name, #product_type_name features or more recalls - top features\n    unsolved- trend history is better but score split betwwen name and score and total performace is lower\n    try differnt parametr for final submission, like more candidates for top sales \n    #checking after label count, to see if can improve the duplications in diffrent weeks (dont think the valid week is strong enough and might confuse the model, maybe cancel duplication leav only highest week?)\n\n    \n    Continuu to experimanet is strtgy ration:\n    try addtional simple one instesd of pairs (same name? last week purchase? based on high features tree score)\n    add addtional stragy that generate like history top n bought by user? like last x purchesed?\n    item_cf\n    customer_cf\n    article mean price and recommnd each customer top sales in customer price category\n# -H&M RAPIDS Article2Vec - short code finds similiar items by vector the description https://www.kaggle.com/code/aerdem4/h-m-rapids-article2vec/notebook\n# - SIMILIAR PICTURE:\n#  FROM EMBEDDING AND knn https://www.kaggle.com/code/joelqv/h-m-product-similarity-1-embeddings-knn/notebook\n# - embedding from torch NN? \n#check also this guy: https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324185\n# Faster and shorter basket anylyssis: https://www.kaggle.com/code/titericz/article-id-pairs-in-3s-using-cudf\n\nfuture to improve code:\n    Build rule collector function\n    maybe add thershhold for positive rate for each rule (run smaller articles number if not meet)","metadata":{"execution":{"iopub.status.busy":"2023-03-24T07:03:39.415336Z","iopub.execute_input":"2023-03-24T07:03:39.415604Z","iopub.status.idle":"2023-03-24T07:03:39.428218Z","shell.execute_reply.started":"2023-03-24T07:03:39.415575Z","shell.execute_reply":"2023-03-24T07:03:39.427054Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reducing memory","metadata":{}},{"cell_type":"code","source":"#old reduction transactions\nid_to_index_dict = dict(zip(customers1[\"customer_id\"].to_pandas(), customers1.index))\nindex_to_id_dict = dict(zip(customers1.index, customers1[\"customer_id\"].to_pandas()))\ntrain[\"customer_id\"] = train[\"customer_id\"].map(id_to_index_dict).astype('int32')\ntrain['article_id'] = train['article_id'].astype('int32')\ntrain['price'] = train['price'].astype('float32')\ntrain['sales_channel_id'] = train['sales_channel_id'].astype('int8')\n\n\n#customer reducing memory old version \ndef dictmap(df, col, typ='int32'):\n    dictval = df[col].unique()\n    cols_to_idx_dict = dict(zip(dictval.to_pandas(), dictval.index))\n    idx_to_cols_dict = dict(zip(dictval.index, dictval.to_pandas()))\n    df[col] = df[col].map(cols_to_idx_dict).astype(typ)\n    return cols_to_idx_dict, idx_to_cols_dict\n\npostal_to_idx_dict, idx_to_postal_dict = dictmap(customers, 'postal_code')\n# custID_to_idx_dict, idx_to_custID = dictmap(customers, 'customer_id')\ncustomers['customer_id'] = customers.customer_id.map(id_to_index_dict).astype('int32') \nfnf_to_idx_dict, idx_to_fnf = dictmap(customers, 'fashion_news_frequency', typ='int8') #reduce more than category \ncms_to_idx_dict, idx_to_cms = dictmap(customers, 'club_member_status', typ='int8')\ncustomers['FN'] = customers.FN.astype('int8')\ncustomers['Active'] = customers.Active.astype('int8')\ncustomers['age'] = customers.age.astype('int8')","metadata":{"execution":{"iopub.status.busy":"2022-08-09T05:43:29.147546Z","iopub.execute_input":"2022-08-09T05:43:29.148272Z","iopub.status.idle":"2022-08-09T05:43:29.187644Z","shell.execute_reply.started":"2022-08-09T05:43:29.148217Z","shell.execute_reply":"2022-08-09T05:43:29.185589Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new final version  - currenlty loading it to save time\n#initial inspiration from Radek H&M startes packs (he use with top samples)\n#https://github.com/radekosmulski/personalized_fashion_recs/blob/main/helper_functions.ipynb\n#old way here under for age, active, FN using map and int8 can reduce more memory (maybe add condition if ints not obj transfer to 'int')\n#Check null cases\n#Maybe can reduce in same cases by half if mapping using negative numbers or u_int(also the mappping has no warning for uniques) - not sure about it\n# Standart memeory reduction by type, min/max vlaues cuold be found in this link: #https://www.kaggle.com/code/sergiomananas/memory-reduction-script\n# whole class need to be refactor (patch with the filter function to avoid building un-necerary dicts, also amany exception in trans, also probably articles can just drop all descriprion that have code)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nclass DataHelper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    missing docstring\n    \"\"\"\n    def __init__(self, min_uniques=10, unchanged_cols=[]):\n        self.min_uniques = min_uniques\n        self.mapdict = {}\n        self.unchanged_cols = unchanged_cols\n\n        \n    def fit(self, df):\n        for col in (x for x in df.columns if self._cols_filter(x, df)):\n            col_unique = df[col].unique()\n            self.mapdict[f'{col}_to_idx_dict'] = dict(zip(col_unique.to_pandas(), col_unique.index))\n            self.mapdict[f'idx_to_{col}_dict'] = dict(zip(col_unique.index,col_unique.to_pandas()))\n        return self\n\n    def _cols_filter(self, x, df):\n        col_condition = ((df[x].nunique()>self.min_uniques)\n                       and (x not in self.unchanged_cols)\n                       )\n        return col_condition\n    \n    def _min_int_for_dict(self, num):\n        if num <= (2 ** 8)/2:\n            typ = 'int8'\n        elif num <= (2 ** 16)/2:\n            typ = 'int16'\n        elif num <= (2 ** 32)/2:\n            typ = 'int32'\n        else:\n            typ = 'int64'   \n        return typ\n    \n    def transform(self, df):\n        start_mem = df.memory_usage(deep=True).sum() / 1024 ** 2\n        for col in df.columns:\n            if col in self.unchanged_cols:\n                continue \n            if df[col].nunique() > self.min_uniques:\n                typ = self._min_int_for_dict(df[col].nunique())\n                df[col] = df[col].map(self.mapdict[f'{col}_to_idx_dict']).astype(typ)\n            else:\n#                 df[col] = df[col].astype('category')  shuold be faster need to update\n                df[col] = pd.Categorical(df[col].to_pandas())\n        end_mem = df.memory_usage(deep=True).sum() / 1024 ** 2\n        percent = 100 * (start_mem - end_mem) / start_mem\n        print(f'Mem. usage decreased from {start_mem:5.2f} Mb to {end_mem:5.2f} Mb ({percent:.1f}% reduction)')\n        return df\n\n#loading original dataset\npath = Path('../input/h-and-m-personalized-fashion-recommendations')\ncustomers = cudf.read_csv(path/'customers.csv', low_memory=False)\narticles = cudf.read_csv(path/'articles.csv', low_memory=False)\ntrans = cudf.read_csv(path/'transactions_train.csv', low_memory=False)#, nrows=1_200_00)\n\n#reducing memory and adding week no.\nDHC = DataHelper(100)\ncustomers = DHC.fit_transform(customers)\n\ndef is_reduntant(x):\n    return ('id' in x)|('no' in x)|('code' in x)\n\nredundant_cols = list(filter(is_reduntant, articles.columns))[1:]\narticles.drop(columns=redundant_cols, inplace=True)\n\nDHA = DataHelper(300)\narticles = DHA.fit_transform(articles)\n\n# Write the whole thing in modern python\nstart_mem = trans.memory_usage(deep=True).sum() / 1024 ** 2\nDHT = DataHelper(300 ,['t_dat', 'customer_id', 'article_id', 'price'])\ntrans = DHT.fit_transform(trans)\n\ntrans['price'] = trans.price.astype('float32')\ntrans['t_dat'] = pd.to_datetime(trans.t_dat.to_pandas())\n\n# typ = DHT._min_int_for_dict(trans['article_id'].nunique()) wont work after trans sample reduce\ntrans['customer_id'] = trans['customer_id'].map(DHC.mapdict['customer_id_to_idx_dict']).astype('int32')\n\n# typ = DHT._min_int_for_dict(trans['article_id'].nunique())\ntrans['article_id'] = trans['article_id'].map(DHA.mapdict['article_id_to_idx_dict']).astype('int32')\n\nend_mem = trans.memory_usage(deep=True).sum() / 1024 ** 2\npercent = 100 * (start_mem - end_mem) / start_mem\nprint(f'Mem. usage Total decreased from {start_mem:5.2f} Mb to {end_mem:5.2f} Mb ({percent:.1f}% reduction)')\n\n#later addtion probably shuold do it after the data load and let datahelper handle it\ntrans['week'] = (trans.t_dat.max() - trans.t_dat).dt.days // 7\ntrans['week'] = trans['week'].astype('int8')\n\n#saving all files and dicts\n\n# customers.to_csv('customers_r.csv')\n# articles.to_csv('articles_r.csv')\n# trans.to_csv('trans_r.csv', chunksize=1_000_000) \n\n# import pickle\n\n# customer_id_to_idx_dict = DHC.mapdict['customer_id_to_idx_dict']\n# idx_to_customer_id_dict = DHC.mapdict['idx_to_customer_id_dict']\n\n# with open('customer_id_to_idx_dict.pkl', 'wb') as handle:\n#     pickle.dump(customer_id_to_idx_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n# with open('idx_to_customer_id_dict.pkl', 'wb') as handle:\n#     pickle.dump(idx_to_customer_id_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n# article_id_to_idx_dict = DHA.mapdict['article_id_to_idx_dict']\n# idx_to_article_id_dict = DHA.mapdict['idx_to_article_id_dict']\n\n# with open('article_id_to_idx_dict.pkl', 'wb') as handle:\n#     pickle.dump(article_id_to_idx_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n# with open('idx_to_article_id_dict.pkl', 'wb') as handle:\n#     pickle.dump(idx_to_article_id_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T06:17:16.794372Z","iopub.execute_input":"2022-12-12T06:17:16.794671Z","iopub.status.idle":"2022-12-12T06:17:16.807910Z","shell.execute_reply.started":"2022-12-12T06:17:16.794638Z","shell.execute_reply":"2022-12-12T06:17:16.806838Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npath = Path('../input/h-and-m-personalized-fashion-recommendations') #for test\npath_reduce = Path('/kaggle/input/hmdata')\ncustomers = cudf.read_csv(path_reduce/'customers_r.csv', index_col=0, low_memory=False)\narticles = cudf.read_csv(path_reduce/'articles_r.csv', index_col=0, low_memory=False)\ntrans = cudf.read_csv(path_reduce/'trans_r.csv', index_col=0, low_memory=False)\n\n# customers memory reduction\ncus_cols = ['FN', 'Active', 'club_member_status','fashion_news_frequency', 'age']\nfor col in cus_cols:\n    customers[col] = customers[col].astype('category')\ncustomers['customer_id'] = customers['customer_id'].astype('int32') \ncustomers['postal_code'] = customers['postal_code'].astype('int32') \nbins = list(range(16,81,4))+ [100]\nnames = list(range(len(bins)-1))\ncustomers = customers.to_pandas()\ncustomers['age_group'] = pd.cut(customers['age'], bins, labels=names, right=False)\ncustomers  = cudf.from_pandas(customers)\n#articles memory reduction\nart_to_cat =  ['product_type_name', 'product_group_name','graphical_appearance_name', 'colour_group_name', \n              'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name', 'index_name', \n              'index_group_name',  'section_name', 'garment_group_name']\nart_to_int = ['article_id', 'prod_name', 'detail_desc']\nfor col in art_to_cat:\n    articles[col] = articles[col].astype('category')\nfor col in art_to_int:\n    articles[col] = articles[col].astype('int32') \n#trans memort reduction\ntrans['price'] = trans.price.astype('float32')\ntrans['t_dat'] = cudf.to_datetime(trans['t_dat'])\ntrans['customer_id'] = trans['customer_id'].astype('int32')\ntrans['article_id'] = trans['article_id'].astype('int32')\ntrans['week'] = trans['week'].astype('int8')\ntrans['sales_channel_id'] = trans['sales_channel_id'].astype('category') #why Cat?","metadata":{"execution":{"iopub.status.busy":"2023-05-25T09:00:29.437706Z","iopub.execute_input":"2023-05-25T09:00:29.438271Z","iopub.status.idle":"2023-05-25T09:00:57.334224Z","shell.execute_reply.started":"2023-05-25T09:00:29.438233Z","shell.execute_reply":"2023-05-25T09:00:57.333381Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"CPU times: user 2.65 s, sys: 1.76 s, total: 4.41 s\nWall time: 27.9 s\n","output_type":"stream"}]},{"cell_type":"code","source":"#reduce to 5% for trial runs\ncustomers = customers.sample(frac=0.05, random_state=42)\ncustlist = set(customers.customer_id.unique().values_host)\ntrans = trans.loc[trans.customer_id.isin(custlist)]\narticleset = set(trans.article_id.values_host)\narticles = articles.loc[articles.article_id.isin(articleset)]","metadata":{"execution":{"iopub.status.busy":"2023-05-03T06:37:58.310764Z","iopub.execute_input":"2023-05-03T06:37:58.315253Z","iopub.status.idle":"2023-05-03T06:37:58.880287Z","shell.execute_reply.started":"2023-05-03T06:37:58.315212Z","shell.execute_reply":"2023-05-03T06:37:58.879532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for final submit only\n#can do for loop with one row\nimport pickle\nwith open(path_reduce/'customer_id_to_idx_dict.pkl', 'rb') as handle:\n    customer_id_to_idx_dict = pickle.load(handle)\nwith open(path_reduce/'idx_to_customer_id_dict.pkl', 'rb') as handle:\n    idx_to_customer_id_dict = pickle.load(handle)\nwith open(path_reduce/'article_id_to_idx_dict.pkl', 'rb') as handle:\n    article_id_to_idx_dict = pickle.load(handle) \nwith open(path_reduce/'idx_to_article_id_dict.pkl', 'rb') as handle:\n    idx_to_article_id_dict = pickle.load(handle)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T09:00:57.338941Z","iopub.execute_input":"2023-05-25T09:00:57.339643Z","iopub.status.idle":"2023-05-25T09:01:00.238427Z","shell.execute_reply.started":"2023-05-25T09:00:57.339597Z","shell.execute_reply":"2023-05-25T09:01:00.237633Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Mapping is usually the best but also int in a few hundreds can be better than category so it can still be optimized but based on the extreme (300 uniques) \n# difference is not so big so currently i dont think need to optimize column by column, here is the fast wat to compare solutions:\n\n#Columns data information:\n# for i in articles.columns:\n#     print(f'{i} has {articles[i].nunique()} and {articles[i].dtype}  ')\n\n#mapping:\n# col='department_name'\n# col_unique = articles[col].unique()\n# Dicti={}\n# Dicti[f'{col}_to_idx_dict'] = dict(zip(col_unique.to_pandas(), col_unique.index))\n# Dicti[f'idx_to_{col}_dict'] = dict(zip(col_unique.index,col_unique.to_pandas()))\n# typ = DataHelper()._min_int_for_dict(articles[col].nunique())\n# articles['dnmmap'] = articles[col].map(Dicti[f'{col}_to_idx_dict']).astype(typ)\n\n#categize:\n# articles['dnmcat'] = pd.Categorical(articles['department_name'].to_pandas())\n\n#intigerize:\n# articles['dnmint'] = articles['department_name'].astype('int16')\n#can reduce also floar in dont need high accuracy like done in price\n\n#---------------------------------------------------------------------------------------------------\n\n#to speed up its better to save all data (dicts and tables) after the reducion\n#here is all the commands, currenlty when reloading the table the type is not the same\n#all total time is 2 minutes can investigate is more in the future, here are the commands:\n# Dicts:\n# import pickle\n\n# def save_object(obj, filename):\n#     with open(filename, 'wb') as outp:  # Overwrites any existing file.\n#         pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n\n# save_object(DHC, 'customers_object.pkl')\n\n# with open('../input/hmcompsolo/customers_object.pkl', 'rb') as inp:\n#     cusobj = pickle.load(inp)\n#tables:\n# customers.to_pandas().to_parquet('cus.pqt') or ('customers.csv') #parquet currently not support Category type\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding features","metadata":{}},{"cell_type":"code","source":"## Articles features\n\n#adding 2-3 features for items:\n#Mean priceVVVVVVVVVVVVVVVVVVVVVVV\n#discountVVVVVVVVVVVVVVVVVVVVVVVVV\n#top K sales? VVVVVVVVVVVVVVVVVVVVVVVVVVV\n#Season by name ('coat', short pants, swim suit...) VVVVVVVVVVVV\n#mean sales per week\n#Changing class LastDiscount to only cudf (has.to_pandas( ) makes it very slow)\n#adding some tqdm and status pring when extractions and collection features\nreview this grouping by age/sex https://www.kaggle.com/code/junjitakeshima/h-m-easy-grouping-by-sex-attribute-age-en-jp\nreview this for addtional features: https://www.kaggle.com/code/iwatatakuya/22nd-place-lgbm-model-single-train","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Union\n\nclass ItemFeatures(ABC):\n    \"\"\" calculate features to articles.\"\"\"\n    @abstractmethod\n    def get(self, *args, **kwargs) -> pd.DataFrame: #change to cudf\n        \"\"\" article_id -> features\"\"\"\n        pass\n\nclass SalesCount(ItemFeatures):\n    \"\"\"\n    count number of transaction  \n    \"\"\"\n    \n    def __init__(self, trans: cudf.DataFrame):\n        self.df = trans\n        pass\n    \n    def get(self):\n        sell_count = (self.df.groupby('article_id')\n            .price\n            .count()\n            .to_frame()\n            .rename(columns  = {'price':'sell_count'})\n        )\n        return sell_count\n\nclass MeanAge(ItemFeatures):\n    \"\"\"\n    mean age of customers per article  \n    \"\"\"\n    \n    def __init__(self, trans: cudf.DataFrame, customers: cudf.DataFrame):\n        self.df = trans.merge(customers[['customer_id', 'age']])\n        self.df['age'] = self.df.age.astype('int8')\n        pass\n    \n    def get(self):\n        mean_age = (self.df.groupby('article_id')\n            .age\n            .mean()\n            .astype('float32')\n            .to_frame()\n            .rename(columns  = {'age':'mean_age'})\n        )\n        return mean_age\n\nclass MeanPrice(ItemFeatures):\n    \"\"\"\n    mean price of item transaction aggrigation \n    \"\"\"\n    \n    def __init__(self, trans: cudf.DataFrame):\n        self.df = trans\n        pass\n    \n    def get(self):\n        mean_price = (self.df.groupby('article_id')\n            .price\n            .mean()\n            .astype('float32')\n            .to_frame()\n            .rename(columns  = {'price':'mean_price'})\n        )\n        return mean_price\n    \n\nclass LastDiscount(ItemFeatures):\n    \"\"\"\n    calcultes items last transaction price vs. highest price\n    \"\"\"\n    \n    def __init__(self, trans: cudf.DataFrame):\n        self.df = trans\n        pass\n\n    \n    def get(self):\n        last_discount = (self.df.to_pandas()\n          .assign(max1=lambda x: x.groupby('article_id')['price'].transform(max))\n          .assign(last_price =lambda x: x.sort_values(by='t_dat', ascending=False )\n                  .groupby('article_id').price\n                  .transform(lambda z: z.iloc[0]) #sort first to verity lat date\n                 )\n          .assign(last_discount=lambda x: x['last_price']/x['max1'])\n          .pipe(lambda x: cudf.from_pandas(x))\n          .pipe(lambda x: x[['article_id', 'last_discount']])\n          .drop_duplicates()\n          .set_index('article_id')\n        )\n        return last_discount   \n    \n    \nclass ItemSeason(ItemFeatures):\n    \"\"\"\n    add summer/winter classification based on key words in the description \n    \"\"\"\n    \n    def __init__(self, articles: cudf.DataFrame, mapdict: Dict ):\n        self.df = articles[['article_id', 'detail_desc']].copy()\n        self.df['detail_desc'] = (\n            self.df['detail_desc']\n            .map(mapdict['idx_to_detail_desc_dict'])\n        )\n        pass\n    \n    \n    def get(self):\n        seasons = self.df.assign(season_type = 0)\n        seasons.loc[seasons.to_pandas().detail_desc.map(self._is_winter, na_action='ignore'), \"season_type\"] = 1\n        seasons.loc[seasons.to_pandas().detail_desc.map(self._is_summer, na_action='ignore'), \"season_type\"] = 2\n        seasons = seasons.set_index('article_id')\n        seasons = seasons.drop(columns = ['detail_desc'])\n        return seasons\n    \n    def _is_winter(self, st: str):\n        winter = {\n         'beanie',\n         'boots',\n         'cardigan',\n         'coat',\n         'earmuffs',\n         'fleece',\n         'gloves',\n         'hoodie',\n         'jacket',\n         'knitted',\n         'long-sleeve',\n         'mittens',\n         'overcoat',\n         'pajamas',\n         'scarf',\n         'ski',\n         'sweater',\n         'winter'\n        }\n        desc = set(st[:-1].lower().split())\n        intersct = len(winter.intersection(desc))\n        return (intersct !=0)\n    \n    def _is_summer(self, st):\n        summer = {\n         'bikini',\n         'cup',\n         'flip',\n         'flops',\n         'hawaiian',\n         'sandals',\n         'sleeveless',\n         'summer',\n         'sunglasses',\n         'swim',\n         'swimsuit',\n         'tank',\n         'tanktop'\n        }\n        desc = set(st[:-1].lower().split())\n        intersct = len(summer.intersection(desc))\n        return (intersct !=0)\n# seasons = ItemSeason(articles, DHA.mapdict)\n\nclass TopK(ItemFeatures):\n    \"\"\"\n    is article in the top k sales\n    \"\"\"\n    \n    def __init__(self, trans: cudf.DataFrame, topk: int):\n        self.df = trans[['article_id']]\n        self.k  = topk\n        pass\n    \n    def get(self):\n        topk = self._top_k_articles()\n        is_top_k = self.df.assign(is_top = 0)\n        is_top_k.loc[is_top_k.article_id.isin(topk), 'is_top'] = 1\n        is_top_k = (\n            is_top_k.rename(columns  = {'is_top': f'is_top_{self.k}'})\n            .drop_duplicates()\n            .set_index('article_id')\n        )\n        return is_top_k\n    \n    def _top_k_articles(self):\n        topk= (\n            self.df\n            .article_id\n            .value_counts()\n            .nlargest(self.k)\n            .index\n            .to_arrow())\n        return topk\n\n\n\nclass FeaturesCollect():\n    \"\"\"add all requested features to the dataframe.\"\"\"  \n    def __init__(self,df: cudf.DataFrame, features: List):\n        self.df  = df\n        self.features  = features\n        pass\n    \n    def collect(self):\n        df_and_features = self.df.copy()\n        for ft in self.features:\n            ftdf = ft.get()\n            df_and_features = df_and_features.merge(ftdf, left_on='article_id', right_index=True, how='left' )\n        return df_and_features        ","metadata":{"execution":{"iopub.status.busy":"2023-05-25T09:01:00.243135Z","iopub.execute_input":"2023-05-25T09:01:00.245216Z","iopub.status.idle":"2023-05-25T09:01:00.289096Z","shell.execute_reply.started":"2023-05-25T09:01:00.245174Z","shell.execute_reply":"2023-05-25T09:01:00.288404Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%time\n# ft = [TopK(trans, 3), TopK(trans, 5), MeanPrice(trans), ItemSeason(articles, DHA.mapdict)] #LastDiscount(trans)]\nft = [SalesCount(trans), MeanPrice(trans), MeanAge(trans, customers)]\nfcl =FeaturesCollect(articles, ft)\narticles2f = fcl.collect()\ndel articles\ndel customers['age'] #move to before merge","metadata":{"execution":{"iopub.status.busy":"2023-05-25T09:01:00.294461Z","iopub.execute_input":"2023-05-25T09:01:00.297026Z","iopub.status.idle":"2023-05-25T09:01:00.572653Z","shell.execute_reply.started":"2023-05-25T09:01:00.296989Z","shell.execute_reply":"2023-05-25T09:01:00.571916Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"CPU times: user 181 ms, sys: 72.8 ms, total: 254 ms\nWall time: 265 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Customers features","metadata":{}},{"cell_type":"code","source":"#Same idea as customers, probably can use same collector \n#need user class or use the class of items?\n#https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/324185 - ךaddtional ideas\n#also have there Customer-item feature!!!\n\n#adding customer features:\n#'cheap' buyer - buys only discounts\n#gender\n#purchase stats (sum, mean.. etc..)\n#re-purchase level","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting CV","metadata":{}},{"cell_type":"markdown","source":"## score metric map@12","metadata":{}},{"cell_type":"code","source":"def apk(actual, predicted, k=12):\n    if len(predicted)>k:\n        predicted = predicted[:k]\n    score = 0.0\n    num_hits = 0.0\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n    if not actual:\n        return 0.0\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=12):\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:30:20.392781Z","iopub.execute_input":"2023-04-27T10:30:20.393073Z","iopub.status.idle":"2023-04-27T10:30:20.400266Z","shell.execute_reply.started":"2023-04-27T10:30:20.393040Z","shell.execute_reply":"2023-04-27T10:30:20.399122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate candidates","metadata":{}},{"cell_type":"markdown","source":"## Retrieval rules","metadata":{}},{"cell_type":"code","source":"from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Union\n\nclass RetrivalRules(ABC):\n    \"\"\" calculte and retrive candidates base on specific rule.\"\"\"\n    @abstractmethod\n    def retrieve(self, *args, **kwargs) -> cudf.DataFrame: \n        \"\"\"df->candidates\"\"\"  \n        pass\n\nclass CustomerHistory2(RetrivalRules):\n    \"\"\"Generate candidates based on the customer transaction History.\"\"\"\n    def __init__(self, trans, n_weeks=4, n_articles=12, valid_week=0):\n        self.name = f'c_history_{n_weeks}w_{n_articles}a'\n        self.n_articles = n_articles\n        self.weekdict = {}\n        df = trans.loc[trans.week>valid_week, ['customer_id', 'article_id', 'week']]\n        for w in range(valid_week+1, n_weeks+valid_week+1):\n            self.weekdict[f'w{w}'] = df.loc[df.week<=w]\n        pass\n    \n    def retrieve(self, cusotmer_list): \n        week_history_dict = self._weekly_sales_dict() \n        cust_best = {}\n        for c in cusotmer_list:\n            for w in week_history_dict:\n                if c in week_history_dict[w]:\n                    tmp = week_history_dict[w][c]\n                    c_top = sorted(tmp.items(), key=lambda item: -item[1])\n                    cust_best[c] = c_top[:self.n_articles]\n                    break\n        k = cust_best.keys() #customers\n        v = cust_best.values() #tuple list (item, score)\n        mut = pd.DataFrame({'customer_id': k, 'article_id': v}).explode('article_id')\n        #splits the tuple value into two separtate columns in the fastest way\n        mut = mut.assign(**dict(zip('ef', zip(*mut.article_id)))).drop(columns=['article_id']).rename(columns={'e':'article_id', 'f':'score'})\n        mut['article_id'] = mut['article_id'].astype('int32')\n        pairs = cudf.from_pandas(mut) \n        pairs[self.name] = np.int8(1)\n        pairs['score'] = pairs['score'].astype('int16') #-----added to reduce memory\n        pairs = pairs.rename(columns = {'score': f'score_{self.name}'})\n        return pairs\n    \n    def _weekly_sales_dict(self):\n        week_history_dict = {}\n        for w, t in self.weekdict.items():\n            week_history_dict[w] = {}\n            for c, a in zip(t.customer_id.values_host, t.article_id.values_host):\n                if c not in week_history_dict[w]:\n                    week_history_dict[w][c] = {}\n                if a not in week_history_dict[w][c]:\n                        week_history_dict[w][c][a] = 0\n                week_history_dict[w][c][a] += 1\n        return week_history_dict\n\n#in order not to build the pairs twice (neg and candidates) need update that it load file if we flag it\nclass basket_analysis(RetrivalRules):\n    \"\"\"return duct of item number and np.array of basket\"\"\"\n    \n    def __init__(self, trans, basket_rank=0, load=True):\n        self.name = f'basket_rank{basket_rank}'\n        self.t = (trans[['customer_id', 'article_id']].\n                  drop_duplicates()\n                 )\n        self.basket_rank = basket_rank\n        self.load = load\n        pass\n\n    def retrieve(self, candidates, t3):\n        if self.load:\n            final_pairs = t3\n        else:\n            final_pairs = self._basket_pairs()\n# need to add save file with stgy name, load file with srgy name if we flag it\n        basket_cadidates = (candidates.merge(final_pairs)\n                            .drop(columns = ['article_id'])\n                            .rename(columns = {'pair_id':'article_id'})\n                           )\n        basket_cadidates = basket_cadidates.groupby(['customer_id', 'article_id']).score.sum().reset_index()\n        basket_cadidates[self.name] = np.int8(1)\n        basket_cadidates['score'] = basket_cadidates['score'].astype('int32') #-----added to reduce memory\n        basket_cadidates = basket_cadidates.rename(columns = {'score': f'score_{self.name}'})\n        return basket_cadidates\n    \n    def _basket_pairs(self):\n        one_timer = (self.t.groupby('article_id')[['customer_id']]\n             .nunique()\n             .query('customer_id==1')\n             .index\n        )\n        to_drop = self.t[self.t.article_id.isin(one_timer)].index\n        t=self.t.drop(to_drop)\n        copy_pair = t.copy()\n        copy_pair.columns = ['customer_id', 'pair_id'] #renaming\n        unique_articles = t.article_id.unique()\n        \n        batch_size = 5000\n        pairs_list = []\n        \n        for i in range(0, len(unique_articles), batch_size):\n            article_batch = unique_articles[i : i+batch_size]\n            pairs = t[t.article_id.isin(article_batch)]\n            pairs = pairs.merge(copy_pair, on=['customer_id'])\n            #remove identical pairs\n            drop_idx = pairs.loc[pairs.article_id==pairs.pair_id].index\n            pairs = pairs.drop(drop_idx)\n            #remove cases of other customers bought just this item\n            c1s = (pairs.groupby(\"article_id\")[[\"customer_id\"]]\n                .nunique()\n                .query(\"customer_id==1\")\n                .index\n                  )\n            single_buy_row_idxs = pairs[pairs[\"article_id\"].isin(c1s)].index\n            pairs = pairs.drop(single_buy_row_idxs)\n            #count top pairs\n            pairs = (pairs.groupby(['article_id', 'pair_id'])\n                     .customer_id\n                     .count()\n                     .reset_index()\n                     .rename(columns={'customer_id':'pair_count'})\n                     .sort_values(by=['article_id', 'pair_count'], ascending = False)\n                    )\n            pairs = pairs[pairs.groupby('article_id').cumcount()==self.basket_rank] #chnge the 0 to if need 2nd one \n            pairs_list.append(pairs)\n        finalpairs = cudf.concat(pairs_list)\n            #normalizing the score\n#         scaler = MinMaxScaler()\n#         finalpairs['pair_count'] = scaler.fit_transform(finalpairs.pair_count.values_host.reshape(-1, 1)).flatten()\n        finalpairs = finalpairs.rename(columns = {'pair_count':'score'})\n        return finalpairs\n\nclass TopSales2(RetrivalRules):\n    \"\"\"Generate candidates based on sales quantity.\"\"\"\n    def __init__(self, trans, n_weeks=4, n_articles=12, valid_week=0):\n        self.name = f'top_sales_{n_weeks}w_{n_articles}a'\n        self.n_articles = n_articles\n        self.df = trans.loc[(trans.week.values>valid_week)&(trans.week.values<(valid_week+n_weeks+1)), ['article_id', 'week']]\n        pass\n\n    def retrieve(self, customer_list):\n        top_series = self.df.article_id.value_counts()[:self.n_articles]\n        top_articles = top_series.index\n        top_scores = top_series.values\n        customers = np.repeat(customer_list, self.n_articles)\n        articles = np.tile(top_articles, len(customer_list))\n        scores = np.tile(top_scores, len(customer_list))\n        top_sells = cudf.DataFrame({'customer_id':customers,\n                                    'article_id': articles,\n                                    f'score_{self.name}':scores})\n        top_sells[self.name] = np.int8(1)\n        return top_sells\n\nclass TopAgeGroup(RetrivalRules):\n    \"\"\"Generate candidtes based on top sales per agegroup\"\"\"\n    def __init__(self, trans, customers, n_weeks=4, n_articles=12, valid_week=0):\n        self.name = f'top_age_group{n_weeks}w_{n_articles}a'\n        self.n_articles = n_articles\n        customers_age_group = customers[['customer_id', 'age_group']]\n        customers_age_group = customers_age_group.dropna(subset=['age_group'])\n        self.customers_age_group = customers_age_group\n        self.df = trans.loc[(trans.week.values>valid_week)&(trans.week.values<(valid_week+n_weeks+1)),['t_dat', 'customer_id', 'article_id']]\n        self.df = cudf.merge(self.df, customers_age_group, on='customer_id')\n        pass\n\n    def retrieve(self, customer_list):\n        top_age = self._top_age()\n        res = pd.DataFrame({'customer_id': customer_list})\n        res = res.merge(self.customers_age_group.to_pandas(), on='customer_id')\n        res = res.merge(top_age, on='age_group', how='left' )\n        res = res.explode('scored_article')\n        res = res.assign(**dict(zip(['article_id', 'score'], zip(*res.scored_article)))).drop(columns=['age_group', 'scored_article'])\n        #https://stackoverflow.com/questions/29550414/how-can-i-split-a-column-of-tuples-in-a-pandas-dataframe\n        res['article_id'] = res['article_id'].astype('int32')\n        res = cudf.from_pandas(res)\n        res[self.name] = np.int8(1)\n        res['score'] = res['score'].astype('int32')\n        res = res.rename(columns = {'score': f'score_{self.name}'})\n        return res\n    \n    def _top_age(self):\n        top_age = self.df.groupby(['age_group', 'article_id'])['t_dat'].count().reset_index().rename(columns = {'t_dat': f'score_age'})\n        top_age = top_age.sort_values(by=['age_group', 'score_age' ], ascending=[True, False])\n        top_age = top_age.to_pandas()\n        top_age = top_age.groupby('age_group').head(self.n_articles)\n        top_age['scored_article'] = list(zip(top_age.article_id, top_age.score_age))\n        top_age = top_age.groupby('age_group').scored_article.apply(list).reset_index()#.rename(columns = {'article_id': 'top_age'})\n        return top_age","metadata":{"execution":{"iopub.status.busy":"2023-05-25T09:01:00.577041Z","iopub.execute_input":"2023-05-25T09:01:00.579143Z","iopub.status.idle":"2023-05-25T09:01:00.650520Z","shell.execute_reply.started":"2023-05-25T09:01:00.579101Z","shell.execute_reply":"2023-05-25T09:01:00.649744Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#rules not in use\nclass TopTrend(RetrivalRules):\n    \"\"\"Generate candidates based on sales trends.\"\"\"\n    def __init__(self, trans, n_weeks=-1, n_articles=12, valid_week=0):\n        self.name = f'top_trend_{n_weeks}w_{n_articles}a'\n        self.n_articles = n_articles\n        self.v_week = valid_week\n        if n_weeks != -1:\n            self.df = trans.loc[(trans.week.values>valid_week)&(trans.week.values<(valid_week+n_weeks+1)), ['article_id', 'week', 't_dat']]\n        else:\n            self.df = trans.loc[(trans.week.values>valid_week), ['article_id', 'week', 't_dat']]\n        pass\n\n    def retrieve(self, customer_list):\n        trends = self._add_trend()\n        top_trends = trends.groupby('article_id')['quotient'].sum()\n        top_trends = top_trends.nlargest(self.n_articles).astype('int32')\n        top_articles = top_trends.index\n        top_scores = top_trends.values\n        customers = np.repeat(customer_list, self.n_articles)\n        articles = np.tile(top_articles, len(customer_list))\n        scores = np.tile(top_scores, len(customer_list))\n        res = cudf.DataFrame({'customer_id':customers,\n                                    'article_id': articles,\n                                    f'score_{self.name}':scores})\n        res[self.name] = np.int8(1)\n        return res\n    \n    def _add_trend(self):\n        weekly_sales = self.df.groupby(['week', 'article_id']).count()\n        weekly_sales = weekly_sales.rename(columns={'t_dat': 'count'})\n        trend_df = self.df.merge(weekly_sales, on=['week', 'article_id'])\n        weekly_sales = weekly_sales.reset_index().set_index('article_id')\n        trend_df = trend_df.merge(\n            weekly_sales.loc[weekly_sales['week']==self.v_week+1, ['count']],\n            on='article_id', suffixes=(\"\", \"_targ\"), how='left')\n        trend_df['count_targ'] = trend_df['count_targ'].fillna(0)\n        trend_df['quotient'] = trend_df['count_targ'] / trend_df['count']\n        return trend_df\n\nclass CustomerTrendHistory(RetrivalRules):\n    \"\"\"Generate candidates based on sales trends.\"\"\"\n    def __init__(self, trans, n_weeks=-1, n_articles=12, valid_week=0):\n        self.name = f'c_trend_history_{n_weeks}w_{n_articles}a'\n        self.n_articles = n_articles\n        self.v_week = valid_week\n        if n_weeks != -1:\n            self.df = trans.loc[(trans.week.values>valid_week)&(trans.week.values<(valid_week+n_weeks+1)), ['article_id','customer_id', 'week', 't_dat']]\n        else:\n            self.df = trans.loc[(trans.week.values>valid_week), ['article_id','customer_id', 'week', 't_dat']]\n        pass\n\n    def retrieve(self, customer_list):\n        df2 = self._add_trend()\n        last_ts = df2['t_dat'].max()\n        tmp = df2.copy().to_pandas()\n        tmp['x'] = ((last_ts - tmp['t_dat']) / np.timedelta64(1, 'D')).astype(int)\n        tmp['dummy_1'] = 1 \n        tmp['x'] = tmp[[\"x\", \"dummy_1\"]].max(axis=1) #replace 0 with 1.3 maybe faster\n\n        a, b, c, d = 2.5e4, 1.5e5, 2e-1, 1e3\n        tmp['y'] = a / np.sqrt(tmp['x']) + b * np.exp(-c*tmp['x']) - d\n\n        tmp['dummy_0'] = 0 \n        tmp['y'] = tmp[[\"y\", \"dummy_0\"]].max(axis=1) #replace below zero with zero\n        tmp['value'] = tmp['quotient'] * tmp['y'] \n#change value to self.nme\n        tmp = tmp.groupby(['customer_id', 'article_id']).agg({'value': 'sum'})\n        tmp = tmp.reset_index()\n        tmp['value'] = tmp['value'].astype('int32')\n        # val = val.query('value>100') must if we use also history top sales\n        tmp = tmp.sort_values(by=['customer_id', 'value'], ascending = False)\n        tmp = tmp.groupby('customer_id').head(self.n_articles)\n        tmp = cudf.from_pandas(tmp)\n        tmp = tmp.loc[tmp.customer_id.isin(customer_list)] #move up to save time(clac only in relevant cust)\n        tmp[self.name] = np.int8(1)\n        a = self.name\n#         tmp = tmp.query('value>0')\n        tmp = tmp.rename(columns = {'value': f'score_{a}'})\n\n        return tmp\n       \n    \n    def _add_trend(self):\n       \n        weekly_sales = self.df.drop('customer_id', axis=1).groupby(['week', 'article_id']).count() #----\n        weekly_sales = weekly_sales.rename(columns={'t_dat': 'count'})\n        \n        trend_df = self.df.merge(weekly_sales, on=['week', 'article_id'])\n        weekly_sales = weekly_sales.reset_index().set_index('article_id')\n        trend_df = trend_df.merge(\n            weekly_sales.loc[weekly_sales['week']==self.v_week+1, ['count']],\n            on='article_id', suffixes=(\"\", \"_targ\"), how='left')\n        trend_df['count_targ'] = trend_df['count_targ'].fillna(0)\n        trend_df['quotient'] = trend_df['count_targ'] / trend_df['count']\n        return trend_df","metadata":{"execution":{"iopub.status.busy":"2023-04-10T09:28:35.327938Z","iopub.execute_input":"2023-04-10T09:28:35.328206Z","iopub.status.idle":"2023-04-10T09:28:35.339358Z","shell.execute_reply.started":"2023-04-10T09:28:35.328176Z","shell.execute_reply":"2023-04-10T09:28:35.338466Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#classes old versions history\n#customer history rule vesions history\n#original first version\nclass CustomerHistory2(RetrivalRules):\n    \"\"\"Generate candidates based on the customer transaction History.\"\"\"\n    def __init__(self, trans, n_weeks=4, n_articles=12, valid_week=0):\n        self.n_articles = n_articles\n        self.valid_week = valid_week\n        df = trans.loc[trans.week>valid_week, ['customer_id', 'article_id', 'week']]\n        self.weekdict = {}\n        for w in range(valid_week+1, n_weeks+valid_week+1):\n            self.weekdict[f'w{w}'] = df.loc[df.week<=w]\n        pass\n    \n    def retrieve(self, cusotmer_list):\n        week_history_dict, week_dummy_dict = self._weekly_sales_dict()\n        cust_best = {}\n        for c in cusotmer_list:\n            for w in week_history_dict:\n                if c in week_history_dict[w]:\n                    tmp = week_history_dict[w][c]\n                    c_top = list((dict(sorted(tmp.items(), key=lambda item: -item[1]))).keys())\n                    if len(c_top) > self.n_articles:\n                        cust_best[c] = c_top[:self.n_articles]\n                    else:\n                        cust_best[c] = c_top + week_dummy_dict[w].values.tolist()[len(c_top):self.n_articles] #chage to latest week\n                    break\n            if c not in cust_best:\n                cust_best[c] = week_dummy_dict['w'+str(self.valid_week+1)].values.tolist()[:self.n_articles]\n        k = cust_best.keys()\n        v = cust_best.values()\n        pairs = (cudf.DataFrame({'customer_id': k, 'article_id': v})\n                 .explode('article_id')\n                 .astype('int32')\n                )\n        return pairs\n    \n    def _weekly_sales_dict(self):\n        week_history_dict = {}\n        week_dummy_dict = {}\n        for w, t in self.weekdict.items():\n            week_dummy_dict[w] = t.article_id.value_counts().nlargest(self.n_articles).index\n            week_history_dict[w] = {}\n            for c, a in zip(t.to_pandas().customer_id, t.to_pandas().article_id):\n                \n                if c not in week_history_dict[w]:\n                    week_history_dict[w][c] = {}\n                    \n                if a not in week_history_dict[w][c]:\n                        week_history_dict[w][c][a] = 0\n                        \n                week_history_dict[w][c][a] += 1\n                \n        return week_history_dict, week_dummy_dict\n\n#old reduced time with top sales\nclass CustomerHistory(RetrivalRules):\n    \"\"\"Generate candidates based on the customer transaction History.\"\"\"\n    def __init__(self, trans, n_weeks=4, n_articles=12, valid_week=0):\n        self.n_articles = n_articles\n        self.valid_week = valid_week\n        df = trans.loc[trans.week>valid_week, ['customer_id', 'article_id', 'week']]\n        df2 = df.loc[df.week<=valid_week+n_weeks].copy()\n        top_n = df2.article_id.value_counts().index[:n_articles].values.tolist()\n        self.top = list(zip(top_n, [0]*n_articles))\n        self.weekdict = {}\n        for w in range(valid_week+1, n_weeks+valid_week+1):\n            self.weekdict[f'w{w}'] = df.loc[df.week<=w]\n        pass\n    \n    def retrieve(self, cusotmer_list):\n        week_history_dict = self._weekly_sales_dict() #, week_dummy_dict\n        cust_best = {}\n        for c in cusotmer_list:\n            for w in week_history_dict:\n                if c in week_history_dict[w]:\n                    tmp = week_history_dict[w][c]\n                    c_top = sorted(tmp.items(), key=lambda item: -item[1])\n                    if len(c_top) > self.n_articles:\n                        cust_best[c] = c_top[:self.n_articles]\n                    else:\n                        cust_best[c] = c_top + self.top[len(c_top):self.n_articles]\n                    break\n            if c not in cust_best:\n                cust_best[c] = self.top[:self.n_articles]\n        k = cust_best.keys() #kustomer\n        v = cust_best.values() #tuple list\n        mut = pd.DataFrame({'customer_id': k, 'article_id': v}).explode('article_id')\n        mut = mut.assign(**dict(zip('ef', zip(*mut.article_id)))).drop(columns=['article_id']).rename(columns={'e':'article_id', 'f':'score'})\n        mut['article_id'] = mut['article_id'].astype('int32')\n        pairs = cudf.DataFrame(mut)\n        return pairs\n    \n    def _weekly_sales_dict(self):\n        week_history_dict = {}\n#         week_dummy_dict = {}\n        for w, t in self.weekdict.items():\n#             week_dummy_dict[w] = t.article_id.value_counts().nlargest(self.n_articles).index\n            week_history_dict[w] = {}\n            for c, a in zip(t.to_pandas().customer_id, t.to_pandas().article_id):\n                \n                if c not in week_history_dict[w]:\n                    week_history_dict[w][c] = {}\n                    \n                if a not in week_history_dict[w][c]:\n                        week_history_dict[w][c][a] = 0\n                        \n                week_history_dict[w][c][a] += 1\n                \n        return week_history_dict#, week_dummy_dict\n\n#new with fetures but slow so change to the one above\nclass CustomerHistory5(RetrivalRules):\n    \"\"\"Generate candidates based on the customer transaction History.\"\"\"\n    def __init__(self, trans, n_weeks=4, n_articles=12, valid_week=0):\n        self.n_articles = n_articles\n        self.valid_week = valid_week\n        self.n_weeks = n_weeks\n        self.df = trans.loc[(trans.week>valid_week)&(trans.week<=(valid_week+n_weeks)), ['customer_id', 'article_id', 'week']]\n        pass\n    \n    def retrieve(self, customer_list):\n        pairs = []\n        top_sales = self.df.article_id.value_counts().nlargest(self.n_articles).reset_index().rename(columns={'index':'article_id', 'article_id':'score'})\n        top_sales['score']=0\n        for c in customer_list:\n            if c in set(self.df.customer_id.values_host):\n                minweek = self.df[self.df.customer_id.values==c].week.min()\n                #next slice actaully shuold be <= minweek to make the same\n                top_c = self.df.loc[(self.df.week.values==minweek)&(self.df.customer_id.values==c)].article_id.value_counts().reset_index().rename(columns={'index':'article_id', 'article_id':'score'})\n                top_c = cudf.concat([top_c, top_sales]).iloc[:self.n_articles]\n                top_c['customer_id'] = c   \n            else:\n                top_c = top_sales.copy()\n                top_c['customer_id'] = c\n            pairs.append(top_c)\n        return cudf.concat(pairs)\n\n#possbly fastest way for this recall from the chosen solution \n\n%%time\nyourList = list(cust_best.keys())\nn = 15\nlen(np.repeat(yourList, n))\n\nfor a in list(dici.values()):\n    d = dict(a)\n#     print(d)\n    print(d.values())\n#     print(d.keys()) #why missing first row?\ndict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\ndict_values([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nimport itertools\nlist(itertools.chain.from_iterable([list(dict(a).values()) for a in dici.values()]))\n# new_list = [x for x in chain.from_iterable(tupList)]\n\nv = [(74739, 4), (57658, 4), (14253, 2), (94674, 1), (57643, 1)]\nprint(dict(v).keys())\nprint(dict(v).values())\n# k = list('abcde')\n#end of customer history version\n\n#basket rule version history and score shift:\n\n#score shift\n#As the transfer score is currently much higher we will temporary  use it\n#in order to create we build the regular cudf using the class, than we build again in the old was creating the dics\n#then we swich score in 2 steps: 1. swich the article_id to the pair_id 2. swich score to the transform Id\n#its because of a mistake in the order in old solution that actially gave a big boost\n#I think that its the new higher score is for more popular items and some how predict better SO:\n# good chance that populrity high selling score will do it as a feature and than we can go back to original score\n\n\n# unhide if its not test or hide if its for test\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\ntest_week = 0\ntrans, test = weeksplit(trans, test_week)\n#end of hide/unhide\n\nba = basket_analysis(trans)\nt3 = ba._basket_pairs()\n\n#new adding score\n#to prevent leakgae split the test first..................\nt = trans[['customer_id', 'article_id']]\nt = t.drop_duplicates()\n#reduce some timer to save memory/increase speed\nonetimer = t.groupby('article_id')[['customer_id']].nunique().query('customer_id==1').index\nto_drop = t[t.article_id.isin(onetimer)].index\nt=t.drop(to_drop)\n\ncopy_pair = t.copy()\ncopy_pair.columns = ['customer_id', 'pair_id']\nunique_articles = t.article_id.unique()\n\nbatch_size = 5000\npairs_list = []\n\nfor i in range(0, len(unique_articles), batch_size):\n    article_batch = unique_articles[i : i+batch_size]\n    pairs = t[t.article_id.isin(article_batch)]\n    pairs = pairs.merge(copy_pair, on=['customer_id'])\n    #remove identical pairs\n    drop_idx = pairs.loc[pairs.article_id==pairs.pair_id].index\n    pairs = pairs.drop(drop_idx)\n#     remove cases of other customers bought just this item\n    c1s = (\n        pairs.groupby(\"article_id\")[[\"customer_id\"]].nunique()\n        .query(\"customer_id==1\").index\n    )\n    single_customer_row_idxs = pairs[pairs[\"article_id\"].isin(c1s)].index\n    pairs = pairs.drop(single_customer_row_idxs)\n    pairs = pairs.groupby(['article_id', 'pair_id']).customer_id.count()\n    pairs = pairs.reset_index().rename(columns={'customer_id':'pair_count'})\n    pairs = pairs.sort_values(by=['article_id', 'pair_count'], ascending = False)\n    pairs = pairs[pairs.groupby('article_id').cumcount()==0] #chnge the 0 to if need 2nd one , verify if shorter than 5\n    pairs_list.append(pairs)\n\nfinalpairs = cudf.concat(pairs_list)\n\n#old with dicts\nfinalpairs = finalpairs.set_index('article_id')\nnew_pairs = finalpairs[\"pair_id\"].to_pandas().to_dict()\n#normlize\nscaler = MinMaxScaler()\nfinalpairs['pair_count'] = scaler.fit_transform(finalpairs.pair_count.values_host.reshape(-1, 1)).flatten() #was 'a'??? try 'change to pair count later'\nnew_score = finalpairs[\"pair_count\"].to_pandas().to_dict()\n\nt3['trans_score'] = t3.pair_id.map(new_score) #running old first\nt3 = t3.drop(columns=['score']).rename(columns = {'trans_score':'score'})\n# t3.to_parquet('t3cv.pqt')\n\n#old original version speed up of weeksplit\nt = trans[['customer_id', 'article_id']]\nt = t.drop_duplicates()\n#reduce some timer to save memory/increase speed\nonetimer = t.groupby('article_id')[['customer_id']].nunique().query('customer_id==1').index\nto_drop = t[t.article_id.isin(onetimer)].index\nt=t.drop(to_drop)\n\ncopy_pair = t.copy()\ncopy_pair.columns = ['customer_id', 'pair_id']\nunique_articles = t.article_id.unique()\n\nbatch_size = 5000\npairs_list = []\n\nfor i in range(0, len(unique_articles), batch_size):\n    article_batch = unique_articles[i : i+batch_size]\n    pairs = t[t.article_id.isin(article_batch)]\n    pairs = pairs.merge(copy_pair, on=['customer_id'])\n    #remove identical pairs\n    drop_idx = pairs.query('article_id==pair_id').index\n    pairs = pairs.drop(drop_idx)\n#     remove one timer to increase speed/save memory\n    c1s = (\n        pairs.groupby(\"article_id\")[[\"customer_id\"]].nunique()\n        .query(\"customer_id==1\").index\n    )\n    single_customer_row_idxs = pairs[pairs[\"article_id\"].isin(c1s)].index\n    pairs = pairs.drop(single_customer_row_idxs)\n\n    pairs = pairs.groupby(['article_id', 'pair_id']).customer_id.count()\n    pairs = pairs.reset_index().rename(columns={'customer_id':'pair_count'})\n    pairs = pairs.sort_values(by=['article_id', 'pair_count'], ascending = False)\n#we want the score!!! we also want stats on pair caunt (avg, count,,.)\n    pairs = pairs.to_pandas().groupby('article_id').head(1) \n    pairs = cudf.DataFrame(pairs.set_index('article_id')[['pair_id']])\n    pairs = cudf.DataFrame(pairs)\n    pairs_list.append(pairs)\n\nfinalpairs = cudf.concat(pairs_list)\n\n# pd.DataFrame({'A':[1,2,3],'B':[4,4,4] })[4:5]\n\n#old original score with strgy func\nscaler = MinMaxScaler()\nfinalpairs['pair_count'] = scaler.fit_transform(finalpairs.pair_count.values_host.reshape(-1, 1)).flatten()\ndef srtgy2(s, finalpairs):\n    a = s.merge(finalpairs).drop(columns = ['article_id']).rename(columns = {'pair_id':'article_id', 'pair_count':'score'})\n    return a\n#end of basket history\n\n#old topsales version history\nclass TopSales(RetrivalRules):\n    \"\"\"Generate candidates based on sales quantity.\"\"\"\n    def __init__(self, trans, n_weeks=4, n_articles=12, valid_week=0):\n        self.n_articles = n_articles\n        self.df = trans.loc[(trans.week.values>valid_week)&(trans.week.values<(valid_week+n_weeks+1)), ['article_id', 'week']]\n        pass\n\n    def retrieve(self, customer_list):\n        #add score - one opton list of tuples with score and explode like history\n        top_list = self.df.article_id.value_counts().index.values[:self.n_articles].tolist()\n        top_sells = cudf.DataFrame()\n        top_sells['customer_id'] = customer_list\n        top_sells['article_id'] = [top_list for x in range(len(customer_list))]\n        top_sells = top_sells.explode('article_id')\n        return top_sells\n#end of topsales history\n\n\n#topagegroup version history--------------------------------------------------\nclass TopAgeGroup3(RetrivalRules):\n    \"\"\"Generate ........\"\"\"\n    def __init__(self, trans, customers, n_weeks=4, n_articles=12, valid_week=0):\n        customers = customers.dropna(subset=['age_group'])\n        self.customers_age_group = customers[['customer_id', 'age_group']]\n        self.name = f'top_age_group{n_weeks}w_{n_articles}a'\n        self.n_articles = n_articles\n        agegroup = trans.loc[(trans.week.values>valid_week)&(trans.week.values<(valid_week+n_weeks+1)),['t_dat', 'customer_id', 'article_id']]\n        agegroup = cudf.merge(agegroup, customers[['customer_id', 'age_group']], on='customer_id')\n        #sub_func\n        ag = agegroup.groupby(['age_group', 'article_id'])['t_dat'].count().reset_index().rename(columns = {'t_dat': f'score_age'})\n        ag = ag.sort_values(by=['age_group', 'score_age' ], ascending=[True, False])\n        ag = ag.to_pandas()\n        ag = ag.groupby('age_group').head(n_articles)\n        ag['scored_article'] = list(zip(ag.article_id, ag.score_age))\n        self.df = ag.groupby('age_group').scored_article.apply(list).reset_index().rename(columns = {'article_id': 'top_age'})\n        pass\n\n    def retrieve(self, customer_list):\n        c = pd.DataFrame({'customer_id': customer_list})\n        c = c.merge(self.customers_age_group.to_pandas(), on='customer_id')\n        c = c.merge(self.df, on='age_group', how='left' )\n        c = c.explode('scored_article')\n        c = c.assign(**dict(zip(['article_id', 'score'], zip(*c.scored_article)))).drop(columns=['age_group', 'scored_article'])\n        #https://stackoverflow.com/questions/29550414/how-can-i-split-a-column-of-tuples-in-a-pandas-dataframe\n        c['article_id'] = c['article_id'].astype('int32')\n        res = cudf.from_pandas(c)\n        res[self.name] = np.int8(1)\n        a = self.name\n        res = res.rename(columns = {'score': f'score_{a}'})\n        return res\n# end o#topagegroup version history--------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2022-11-10T07:19:45.837060Z","iopub.execute_input":"2022-11-10T07:19:45.837485Z","iopub.status.idle":"2022-11-10T07:19:45.859576Z","shell.execute_reply.started":"2022-11-10T07:19:45.837389Z","shell.execute_reply":"2022-11-10T07:19:45.858259Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#unit test for class changes\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\nvalid_week = 0\ntrain, valid = weeksplit(trans, valid_week)\nvalid_customers = valid.customer_id.unique().values_host\n# CHrate = CustomerHistory2(trans, 4, 15, valid_week)\n# recalls1 = CHrate.retrieve(valid_customers)\ntagc = TopAgeGroup(trans, customers, n_weeks=1, n_articles=15, valid_week=valid_week)\nrecalls5 = tagc.retrieve(valid_customers)\n\ntagc3 = TopAgeGroup3(trans, customers, n_weeks=1, n_articles=15, valid_week=valid_week)\nrecalls53 = tagc3.retrieve(valid_customers)\n\n#check baskets\nt4 = cudf.read_parquet(path_reduce/'t0sub.pqt')\nt5 = cudf.read_parquet(path_reduce/'t1sub.pqt') \n\nBA12 = basket_analysis(trans) #send all data include all weeks except test\nrecalls22 = BA12.retrieve(recalls1[['customer_id','article_id']], t4) ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T09:59:50.808614Z","iopub.execute_input":"2023-04-19T09:59:50.809469Z","iopub.status.idle":"2023-04-19T09:59:50.813760Z","shell.execute_reply.started":"2023-04-19T09:59:50.809431Z","shell.execute_reply":"2023-04-19T09:59:50.812619Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rule checker\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\ndef rule_score(actual, predict):\n    actual = actual[['customer_id', 'article_id']].drop_duplicates()\n    act_tot = len(actual)\n    pre_tot = len(predict)\n    correct = actual.merge(predict, on=['customer_id', 'article_id'], how='inner').shape[0]\n    print(f\"[+] Recall = {correct/act_tot*100:.4f}% ({correct}/{act_tot})\")\n    print(f\"[+] Precision = {correct/pre_tot*100:.4f}% ({correct}/{pre_tot})\")\n    return f\"{correct/act_tot*100:.4f}%\",  f'({correct}/{act_tot})', f\"{correct/pre_tot*100:.4f}%\",  f'({correct}/{pre_tot})'\n\n#old recall\n# def recall_ratio(recalls, valid):\n#     \"get recalls and valid - returs the postive rate\"\n#     buys = valid[['customer_id', 'article_id']].drop_duplicates()\n#     buys['label']=1\n#     pos_rate = recalls.merge(buys, on=['customer_id', 'article_id'], how='left').label.fillna(0).mean()\n#     return pos_rate\n\n# recall_ratio(recalls, valid) #0.0089 (w0), 0.0078(w1), 0.0077(w2), 0.0068(w3) - min_pos_rate=0.006\n\nt4 = cudf.read_parquet(path_reduce/'t0sub.pqt')\nt5 = cudf.read_parquet(path_reduce/'t1sub.pqt') \n# t4 = t4.loc[t4.score>10]\n# t4_high_score.sort_values(by=['pair_id', 'article_id'])[:30]\n\n%%time\nrecall_list = []; recall_ver_l = []; precision_list = []; precision_ver_list = []\nfor valid_week in [0, 1, 2]:#, 1, 2]:#, 3, 4, 5, 6, 7, 8, 9]:\n    print(f'this is for valid week{valid_week}')\n    train, valid = weeksplit(trans, valid_week)\n    valid_customers = valid.customer_id.unique().values_host\n    CHrate = CustomerHistory2(trans, 4, 15, valid_week)\n    recalls1 = CHrate.retrieve(valid_customers)\n    \n#     cth = CustomerTrendHistory(trans, n_weeks=4, n_articles=15, valid_week=valid_week)\n#     recalls10 = cth.retrieve(valid_customers)\n    \n    \n    \n    BA1 = basket_analysis(trans) #send all data include all weeks except test\n    recalls2 = BA1.retrieve(recalls1[['customer_id','article_id']], t4) \n    \n#     recalls2 = recalls2.drop_duplicates(['customer_id', 'article_id'])\n    recalls4 = BA1.retrieve(recalls1[['customer_id','article_id']], t5)\n     \n\n    ts = TopSales2(trans, n_weeks=1, n_articles=30, valid_week=valid_week)\n    recalls3 = ts.retrieve(valid_customers)\n    \n    ts2 = TopSales2(trans, n_weeks=10, n_articles=10, valid_week=valid_week)\n    recalls7 = ts2.retrieve(valid_customers)\n\n    tagc = TopAgeGroup(trans, customers, n_weeks=1, n_articles=15, valid_week=valid_week)\n    recalls5 = tagc.retrieve(valid_customers)\n    \n#     tpf = TopTrend(trans, n_weeks=-1, n_articles=10, valid_week=valid_week)\n#     recalls6 = tpf.retrieve(valid_customers)\n   \n    recallsmrg = recalls1.copy()\n    for rec3 in [recalls2, recalls3, recalls4, recalls5, recalls7]:\n        before_merge = recallsmrg.shape[0] #to delete\n        union = recallsmrg.merge(rec3, on=['customer_id', 'article_id'], how='inner')\n        recallsmrg = recallsmrg.merge(rec3, on=['customer_id', 'article_id'], how='outer')\n        x = recallsmrg.shape[0] - before_merge - rec3.shape[0] + union.shape[0]\n        print(f'no duplicate verification {x}')\n    print(f'total merge {recallsmrg.shape}')\n    \n    rec, rec_v, pre, pre_v = rule_score(valid, recallsmrg)\n    recall_list.append(rec); recall_ver_l.append(rec_v); precision_list.append(pre); precision_ver_list.append(pre_v)\n    print('\\n')\n\npd.DataFrame([recall_list, recall_ver_l, precision_list, precision_ver_list])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-28T06:38:06.157054Z","iopub.execute_input":"2023-03-28T06:38:06.157387Z","iopub.status.idle":"2023-03-28T06:38:06.170859Z","shell.execute_reply.started":"2023-03-28T06:38:06.157268Z","shell.execute_reply":"2023-03-28T06:38:06.170255Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Intenal CV","metadata":{}},{"cell_type":"code","source":"def weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\ntest_week = 0\ntrans, test = weeksplit(trans, test_week)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:30:31.468255Z","iopub.execute_input":"2023-04-27T10:30:31.468525Z","iopub.status.idle":"2023-04-27T10:30:31.480208Z","shell.execute_reply.started":"2023-04-27T10:30:31.468494Z","shell.execute_reply":"2023-04-27T10:30:31.479463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# * WEEK_NUM = 0: test\n# * WEEK_NUM = 1, 2: valid\n# * WEEK_NUM > 1, 2: train\n#split test and train\n#split again to train and valid\n\n#old with 2nd recall stg and score + normilize\nrecalls = []\nfor valid_week in [1, 2, 3]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host \n    CH1 = CustomerHistory(trans, 4, 15, valid_week)\n    negdf1 = CH1.retrieve(custlist)\n    negdf1['score'] = scaler.fit_transform(negdf1.score.values_host.reshape(-1, 1)).flatten()\n    negdf1['strtgy1'] = np.int8(1)\n    negdf2 = srtgy2(negdf1.query('score>0')[['customer_id','article_id']], finalpairs)\n    \n#     negdf2 = negdf1.copy()\n#     negdf2 = negdf2.loc[negdf2.score.values>0]     \n#     negdf2['score'] = negdf2.article_id.map(new_score)\n#     negdf2['article_id'] = negdf2.article_id.map(new_pairs)\n    negdf2['strtgy2'] = np.int8(1)\n# #     negdf2['strtgy1'] = np.int8(0)\n# #     negdf2 = negdf2.drop_duplicates()\n    negdf = cudf.concat([negdf1, negdf2]).fillna(0)\n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='outer').fillna({'label':0}) #change to inner or left\n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)\n   \n\n\n#old with 2nd recall stg and score + normilize + class\nrecalls = []\nfor valid_week in [1, 2, 3]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host \n    CH1 = CustomerHistory(trans, 4, 15, valid_week)\n    negdf1 = CH1.retrieve(custlist)\n    scaler = MinMaxScaler()\n    negdf1['score'] = scaler.fit_transform(negdf1.score.values_host.reshape(-1, 1)).flatten() #move to class\n    negdf1['strtgy1'] = np.int8(1)\n    BA1 = basket_analysis(trans) #send all data include all weeks except test\n    negdf2 = BA1.retrieve(negdf1.query('score>0')[['customer_id','article_id']], t3)\n#     negdf2 = srtgy2(negdf1.query('score>0')[['customer_id','article_id']], finalpairs)\n#     negdf2 = negdf1.copy()\n#     negdf2 = negdf2.loc[negdf2.score.values>0]     \n#     negdf2['score'] = negdf2.article_id.map(new_score)\n#     negdf2['article_id'] = negdf2.article_id.map(new_pairs)\n    negdf2['strtgy2'] = np.int8(1)\n# #     negdf2['strtgy1'] = np.int8(0)\n# #     negdf2 = negdf2.drop_duplicates()\n    negdf = cudf.concat([negdf1, negdf2]).fillna(0)\n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='outer').fillna({'label':0}) #change to inner or left\n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)\n\n#old with 2nd history and top split\nrecalls = []\nfor valid_week in [1, 2, 3]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host \n    CH1 = CustomerHistory2(trans, 4, 15, valid_week) #ch to 2\n    negdf1 = CH1.retrieve(custlist)\n    scaler = MinMaxScaler()\n#     negdf1['score'] = scaler.fit_transform(negdf1.score.values_host.reshape(-1, 1)).flatten() #move to class\n    #     negdf1 = negdf1.rename(columns={\"score\": \"st1_score\"})\n    negdf1['strtgy1'] = np.int8(1)\n    BA1 = basket_analysis(trans) #send all data include all weeks except test\n    negdf2 = BA1.retrieve(negdf1[['customer_id','article_id']], t3) #didnt filter the zero\n    negdf2['strtgy2'] = np.int8(1)\n    ts = TopSales(trans, n_weeks=1, n_articles=30, valid_week=valid_week)\n    negdf3 = ts.retrieve(custlist)\n    negdf3['strtgy3'] = np.int8(1)\n    negdf3['score'] = np.int8(0)\n    #     negdf3['st3_score'] = np.int8(0)\n\n    negdf = cudf.concat([negdf1, negdf2, negdf3]).fillna(0)\n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='outer').fillna({'label':0}) #change to  left\n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T11:36:16.926366Z","iopub.execute_input":"2022-12-05T11:36:16.926809Z","iopub.status.idle":"2022-12-05T11:36:32.726645Z","shell.execute_reply.started":"2022-12-05T11:36:16.926773Z","shell.execute_reply":"2022-12-05T11:36:32.725919Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#winner\n\nt4 = cudf.read_parquet(path_reduce/'t0cv.pqt') #'t0sub.pqt' -for test, and t0cv.pqt-for internal cv\nt5 = cudf.read_parquet(path_reduce/'t1cv.pqt') #'t1sub.pqt' -for test, and t1cv.pqt-for internal cv\n\nrecalls = []\nfor valid_week in [1, 2, 3]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host  \n    CH1 = CustomerHistory2(trans, 4, 15, valid_week) \n    negdf1 = CH1.retrieve(custlist)\n\n    BA1 = basket_analysis(trans) #send all data include all weeks except test\n    negdf2 = BA1.retrieve(negdf1[['customer_id','article_id']], t4) #didnt filter the zero\n    negdf5 = BA1.retrieve(negdf1[['customer_id','article_id']], t5) \n    negdf2 = cudf.concat([negdf2, negdf5])\n    negdf2 = negdf2.drop_duplicates(['customer_id', 'article_id']) #check regaring the name and score\n    \n    ts = TopSales2(trans, n_weeks=1, n_articles=30, valid_week=valid_week)\n    negdf3 = ts.retrieve(custlist)\n    \n    ts2 = TopSales2(trans, n_weeks=10, n_articles=10, valid_week=valid_week)\n    negdf7 = ts2.retrieve(custlist)\n    \n    tag = TopAgeGroup(trans, customers, n_weeks=1, n_articles=15, valid_week=valid_week) \n    negdf4 = tag.retrieve(custlist)\n   \n    negdf = negdf1.copy()\n    for neg in [negdf2, negdf3, negdf4, negdf7]:\n        negdf = negdf.merge(neg, on=['customer_id', 'article_id'], how='outer')\n    negdf = negdf.fillna(0)\n    strgy_names = [x for x in negdf.columns if 'score' not in x][2:]\n    negdf['strgy_count'] = negdf[strgy_names].sum(axis=1).astype('int8')\n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='left').fillna({'label':0}) \n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)\n\ntrainfin = cudf.concat(recalls) #keep\ntrainfin = (trainfin\n     .merge(customers, on='customer_id')\n     .merge(articles2f, on='article_id') #change to aff features\n)\n\nprint(trainfin.label.value_counts(dropna=False))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:30:37.342893Z","iopub.execute_input":"2023-04-27T10:30:37.343169Z","iopub.status.idle":"2023-04-27T10:30:41.575588Z","shell.execute_reply.started":"2023-04-27T10:30:37.343137Z","shell.execute_reply":"2023-04-27T10:30:41.574696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training lightgbm ranker final\ntrainfin = trainfin.to_pandas() #check if cuold avoid\ngc.collect()\n\nfrom lightgbm.sklearn import LGBMRanker\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    max_depth=7,\n    n_estimators=300,\n    importance_type='gain',\n    verbose=10\n)\n\n# trainfin.sort_values(['customer_id', 'valid_week'], inplace = True)\n# train_baskets = trainfin.groupby(['customer_id', 'valid_week'])['article_id'].count().values\n\ntrainfin.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets = trainfin.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\nranker = ranker.fit(\n    trainfin.drop(columns = [ 'customer_id', 'article_id', 'label']),#added valid week----\n    trainfin.pop('label'),\n    group=train_baskets,\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:30:47.399126Z","iopub.execute_input":"2023-04-27T10:30:47.399427Z","iopub.status.idle":"2023-04-27T10:33:00.674309Z","shell.execute_reply.started":"2023-04-27T10:30:47.399392Z","shell.execute_reply":"2023-04-27T10:33:00.673561Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainfin playground with eval\n\ntrainfin = trainfin.to_pandas()\ntrainfin_train = trainfin.loc[trainfin.valid_week<3].copy()\ntrainfin_valid = trainfin.loc[trainfin.valid_week==3].copy()\n\ntrainfin_train.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets_qids = trainfin_train.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\ntrainfin_valid.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets_qids_valid = trainfin_valid.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\nfrom lightgbm.sklearn import LGBMRanker\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    max_depth=7,\n    n_estimators=300,\n    importance_type='gain',\n    verbose=10\n)\n\nranker = ranker.fit(\n    X=trainfin_train.drop(columns = [ 'customer_id', 'article_id', 'label']),\n    y=trainfin_train.pop('label'),\n    group=train_baskets_qids,\n    eval_set=[(trainfin_valid.drop(columns = [ 'customer_id', 'article_id', 'label']), trainfin_valid.pop('label'))],\n    eval_group=[train_baskets_qids_valid],\n    eval_at=10,\n    verbose=10,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T11:52:01.194063Z","iopub.execute_input":"2023-03-30T11:52:01.194337Z","iopub.status.idle":"2023-03-30T11:52:01.294104Z","shell.execute_reply.started":"2023-03-30T11:52:01.194306Z","shell.execute_reply":"2023-03-30T11:52:01.293363Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp = (ranker.feature_importances_ / ranker.feature_importances_.sum())*100\ndesc = ranker.feature_name_\npd.DataFrame({'name': desc, 'score': imp}).sort_values(by=['score'], ascending=False)\n# ranker.evals_result_","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:07:32.636620Z","iopub.execute_input":"2023-05-03T08:07:32.636938Z","iopub.status.idle":"2023-05-03T08:07:32.661683Z","shell.execute_reply.started":"2023-05-03T08:07:32.636899Z","shell.execute_reply":"2023-05-03T08:07:32.660855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#old with score and week\ncustlist3 = test.customer_id.unique().values_host \nCHv = CustomerHistory(trans, 4, 15) #defaoult valid week is 0 so its taking only train\ncandidates = CHv.retrieve(custlist3)\ncandidates['valid_week'] = test_week\n\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n\n#adjusting features\ncandidates =  candidates[['valid_week', 'customer_id', 'article_id','FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age','product_type_name', 'product_group_name',\n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name', 'perceived_colour_master_name',\n       'department_name', 'index_name', 'index_group_name', 'section_name',\n       'garment_group_name','score']]\n#----------------------------------------------------------------------\n\n# old 2 strgy\ncustlist3 = test.customer_id.unique().values_host \nCHv1 = CustomerHistory(trans, 4, 15)\n# CHv2 = CustomerHistory(trans, 5, 15)#defaoult valid week is 0 so its taking only train\ncandidates1 = CHv1.retrieve(custlist3)\ncandidates1['strtgy1'] = np.int8(1)\n# candidates2 = CHv2.retrieve(custlist3)\ncandidates2 = candidates1.copy()\ncandidates2 = candidates2.loc[candidates2.score.values>0]    \ncandidates2['article_id'] = candidates2.article_id.map(new_pairs)\ncandidates2['strtgy2'] = np.int8(1)\n\n\ncandidates = cudf.concat([candidates1, candidates2]).fillna(0)\ncandidates['valid_week'] = test_week\n\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n\n#adjusting features\ncandidates =  candidates[['valid_week', 'customer_id', 'article_id','FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age','product_type_name', 'product_group_name',\n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name', 'perceived_colour_master_name',\n       'department_name', 'index_name', 'index_group_name', 'section_name',\n       'garment_group_name','score', 'strtgy1', 'strtgy2']] #added strgy\n#---------------------------------------------------------------------------------------------------------\n\n#old 2 strgy +2nd score normlize\ncustlist3 = test.customer_id.unique().values_host \nCHv1 = CustomerHistory(trans, 4, 15)\n\ncandidates1 = CHv1.retrieve(custlist3)\ncandidates1['score'] = scaler.fit_transform(candidates1.score.values_host.reshape(-1, 1)).flatten()\ncandidates1['strtgy1'] = np.int8(1)\n# candidates2 = CHv2.retrieve(custlist3)\n# candidates2 = srtgy2(candidates1.article_id, finalpairs)\ncandidates2 = srtgy2(candidates1.query('score>0')[['customer_id','article_id']], finalpairs)\n\n# candidates2 = candidates1.copy()\n# candidates2 = candidates2.loc[candidates2.score.values>0]    \n# candidates2['article_id'] = candidates2.article_id.map(new_pairs)\n# candidates2['score'] = candidates2.article_id.map(new_score)\ncandidates2['strtgy2'] = np.int8(1)\n\n\ncandidates = cudf.concat([candidates1, candidates2]).fillna(0)\ncandidates['valid_week'] = test_week\n\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n\n#add drop duplicate\n\n#adjusting features\ncandidates =  candidates[['valid_week', 'customer_id', 'article_id','FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age','product_type_name', 'product_group_name',\n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name', 'perceived_colour_master_name',\n       'department_name', 'index_name', 'index_group_name', 'section_name',\n       'garment_group_name','score', 'strtgy1', 'strtgy2']] #added strgy\n#-------------------------------------------------------------------------------------------------------\n\n#old 2 strgy +2nd score normlize +class\n#to aff features need change merge and columns namnes \ncustlist3 = test.customer_id.unique().values_host \nCHv1 = CustomerHistory(trans, 4, 15)\n\ncandidates1 = CHv1.retrieve(custlist3)\ncandidates1['score'] = scaler.fit_transform(candidates1.score.values_host.reshape(-1, 1)).flatten()\ncandidates1['strtgy1'] = np.int8(1)\n\nBAv1 = basket_analysis(trans) #2nd time calculte the pair, need to add loading option\ncandidates2 = BAv1.retrieve(candidates1.query('score>0')[['customer_id','article_id']], t3)\n# candidates2 = srtgy2(candidates1.query('score>0')[['customer_id','article_id']], finalpairs)\n# candidates2 = candidates1.copy()\n# candidates2 = candidates2.loc[candidates2.score.values>0]    \n# candidates2['article_id'] = candidates2.article_id.map(new_pairs)\n# candidates2['score'] = candidates2.article_id.map(new_score)\ncandidates2['strtgy2'] = np.int8(1)\ncandidates = cudf.concat([candidates1, candidates2]).fillna(0)\ncandidates['valid_week'] = test_week\n\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id')) #change to add featires\n)\n\n#add drop duplicate\n\n#adjusting features\n# candidates =  candidates[['valid_week', 'customer_id', 'article_id','FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age','product_type_name', 'product_group_name',\n#        'graphical_appearance_name', 'colour_group_name',\n#        'perceived_colour_value_name', 'perceived_colour_master_name',\n#        'department_name', 'index_name', 'index_group_name', 'section_name',\n#        'garment_group_name','score', 'strtgy1', 'strtgy2']] #change to add features\n\n#----------------------------------------------------------------------------------------------------------------\n\n#old 2 strgy history and topsales------ concat and adding manually score and names \ncustlist3 = test.customer_id.unique().values_host \nCHv1 = CustomerHistory2(trans, 4, 15)\ncandidates1 = CHv1.retrieve(custlist3)\n# candidates1['score'] = scaler.fit_transform(candidates1.score.values_host.reshape(-1, 1)).flatten()\ncandidates1['strtgy1'] = np.int8(1)\n\nBAv1 = basket_analysis(trans) #2nd time calculte the pair, need to add loading option\ncandidates2 = BAv1.retrieve(candidates1[['customer_id','article_id']], t3)\ncandidates2['strtgy2'] = np.int8(1)\n\nts1 = TopSales(trans, n_weeks=1, n_articles=30)\ncandidates3 = ts1.retrieve(custlist3)\ncandidates3['strtgy3'] = np.int8(1)\ncandidates3['score'] = np.int8(0)\n\n\ncandidates = cudf.concat([candidates1, candidates2, candidates3]).fillna(0)\ncandidates['valid_week'] = test_week\n\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id')) #change to add featires\n)\n\n#add drop duplicate\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T07:01:16.041963Z","iopub.execute_input":"2022-11-15T07:01:16.043735Z","iopub.status.idle":"2022-11-15T07:01:20.583898Z","shell.execute_reply.started":"2022-11-15T07:01:16.043696Z","shell.execute_reply":"2022-11-15T07:01:20.583168Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#winner\ncustlist3 = test.customer_id.unique().values_host \nCHv1 = CustomerHistory2(trans, 4, 15) #valid_week=0 by default  - change for readabilty \ncandidates1 = CHv1.retrieve(custlist3)\n\nBAv1 = basket_analysis(trans) #2nd time calculte the pair, need to add loading option\ncandidates2 = BAv1.retrieve(candidates1[['customer_id','article_id']], t4)\ncandidates5 = BAv1.retrieve(candidates1[['customer_id','article_id']], t5) \ncandidates2 = cudf.concat([candidates2, candidates5])\ncandidates2 = candidates2.drop_duplicates(['customer_id', 'article_id'])\n \nts1 = TopSales2(trans, n_weeks=1, n_articles=30) #valid_week=0 by default  - change for readabilty \ncandidates3 = ts1.retrieve(custlist3)\n\nts2 = TopSales2(trans, n_weeks=10, n_articles=10) #valid_week=0 by default  - change for readabilty \ncandidates7 = ts2.retrieve(custlist3)\n\ntagv1 = TopAgeGroup(trans, customers, n_weeks=1, n_articles=15, valid_week=0)\ncandidates4 = tagv1.retrieve(custlist3)\n\ncandidates = candidates1.copy()\nfor can in [candidates2,  candidates3,  candidates4, candidates7]:\n    candidates = candidates.merge(can, on=['customer_id', 'article_id'], how='outer')\ncandidates = candidates.fillna(0)\nstrgy_names = [x for x in candidates.columns if 'score' not in x][2:] #prbly can cancel\ncandidates['strgy_count'] = candidates[strgy_names].sum(axis=1).astype('int8')\n    \ncandidates['valid_week'] = np.int8(test_week) \n\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles2f, on = ('article_id')) \n)\n#add drop duplicate","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:33:00.676271Z","iopub.execute_input":"2023-04-27T10:33:00.676853Z","iopub.status.idle":"2023-04-27T10:33:01.037811Z","shell.execute_reply.started":"2023-04-27T10:33:00.676807Z","shell.execute_reply":"2023-04-27T10:33:01.036852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#winner\npreds = []\nbatch_size = 10_000\nfor bucket in tqdm(range(0, len(candidates), batch_size)):\n  outputs = ranker.predict(\n      candidates.iloc[bucket: bucket+batch_size]\n      .drop(columns = ['customer_id', 'article_id'])\n      .to_pandas()\n      )\n  preds.append(outputs)\n\npreds = np.concatenate(preds)\ncandidates['preds'] = preds\npreds = candidates[['customer_id', 'article_id', 'preds']]\npreds = preds.to_pandas() #check if cuold avoid\npreds.sort_values(['customer_id', 'preds'], ascending=False, inplace = True) #can be done with cudf\npreds2 = preds.groupby('customer_id').article_id.apply(list).reset_index() #must change pandas (not ittreble)\ntest = test.to_pandas() #check if cuold avoid\nval = test.groupby('customer_id').article_id.apply(list).reset_index()\nss = val.merge(preds2, left_on='customer_id', right_on='customer_id')\nmapk(ss['article_id_x'], ss['article_id_y'], k=12)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:33:01.039130Z","iopub.execute_input":"2023-04-27T10:33:01.039833Z","iopub.status.idle":"2023-04-27T10:33:05.117510Z","shell.execute_reply.started":"2023-04-27T10:33:01.039790Z","shell.execute_reply":"2023-04-27T10:33:05.116718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = cudf.from_pandas(test)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T11:25:03.299062Z","iopub.execute_input":"2023-04-20T11:25:03.299345Z","iopub.status.idle":"2023-04-20T11:25:03.310174Z","shell.execute_reply.started":"2023-04-20T11:25:03.299312Z","shell.execute_reply":"2023-04-20T11:25:03.309352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Private leaderboard  - new","metadata":{}},{"cell_type":"code","source":"#old - but cahnged a little in the middle\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\n# test_week = 0\n# trans, test = weeksplit(trans, test_week)\n\nrecalls = []\nfor valid_week in [0, 1, 2]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host \n    CH = CustomerHistory(trans, 4, 15, valid_week)\n    negdf1 = CH.retrieve(custlist)\n    scaler = MinMaxScaler()\n    negdf1['score'] = scaler.fit_transform(negdf1.score.values_host.reshape(-1, 1)).flatten()\n    negdf1['strtgy1'] = np.int8(1)\n    negdf2 = negdf1.copy()\n    negdf2 = negdf2.loc[negdf2.score.values>0]     \n    \n    negdf2['score'] = negdf2.article_id.map(new_score) #chnged to correct score check if its really so low\n    negdf2['article_id'] = negdf2.article_id.map(new_pairs)\n    \n#     negdf2['article_id'] = negdf2.article_id.map(new_pairs)\n#     negdf2['score'] = negdf2.article_id.map(new_score)\n    \n    negdf2['strtgy2'] = np.int8(1)\n    negdf = cudf.concat([negdf1, negdf2]).fillna(0)\n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='outer').fillna({'label':0})\n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)\n\n#added valid score\ntrainfin = cudf.concat(recalls)\ntrainfin = trainfin[trainfin['score'].notna()] #only recalls no positive from weekly valid\n\ntrainfin = (trainfin\n     .merge(customers, on='customer_id')\n     .merge(articles, on='article_id')\n)\n\ntrainfin =  trainfin[['valid_week', 'customer_id', 'article_id','FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age','product_type_name', 'product_group_name',\n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name', 'perceived_colour_master_name',\n       'department_name', 'index_name', 'index_group_name', 'section_name',\n       'garment_group_name','label', 'score', 'strtgy1', 'strtgy2']]\n\ntrainfin = trainfin.to_pandas() #check if cuold avoid\ngc.collect()\n\n#training lightgbm ranker ---new\n#added valid week sort and gruup\nfrom lightgbm.sklearn import LGBMRanker\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    max_depth=7,\n    n_estimators=300,\n    importance_type='gain',\n    verbose=10\n)\n\n# trainfin.sort_values(['customer_id', 'valid_week'], inplace = True)\n# train_baskets = trainfin.groupby(['customer_id', 'valid_week'])['article_id'].count().values\n\ntrainfin.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets = trainfin.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\nranker = ranker.fit(\n    trainfin.drop(columns = [ 'customer_id', 'article_id', 'label']),\n    trainfin.pop('label'),\n    group=train_baskets,\n)\n\n#----history is combined----------------------------------------------------------------------------------------------------\n#old\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\n# test_week = 0\n# trans, test = weeksplit(trans, test_week)\n\nrecalls = []\nfor valid_week in [0, 1, 2]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host \n    CH = CustomerHistory(trans, 4, 15, valid_week)\n    negdf1 = CH.retrieve(custlist)\n    scaler = MinMaxScaler()\n    negdf1['score'] = scaler.fit_transform(negdf1.score.values_host.reshape(-1, 1)).flatten()\n    negdf1['strtgy1'] = np.int8(1)\n    \n    BA1 = basket_analysis(trans) #send all data include all weeks except test\n    negdf2 = BA1.retrieve(negdf1.query('score>0')[['customer_id','article_id']], t3)\n    negdf2['strtgy2'] = np.int8(1)\n    \n    negdf = cudf.concat([negdf1, negdf2]).fillna(0)\n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='outer').fillna({'label':0})\n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)\n\n#added valid score\ntrainfin = cudf.concat(recalls)\ntrainfin = trainfin[trainfin['score'].notna()] #only recalls no positive from weekly valid\n\ntrainfin = (trainfin\n     .merge(customers, on='customer_id')\n     .merge(articles, on='article_id')\n)\n\ntrainfin =  trainfin[['valid_week', 'customer_id', 'article_id','FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age','product_type_name', 'product_group_name',\n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name', 'perceived_colour_master_name',\n       'department_name', 'index_name', 'index_group_name', 'section_name',\n       'garment_group_name','label', 'score', 'strtgy1', 'strtgy2']]\n\ntrainfin = trainfin.to_pandas() #check if cuold avoid\ngc.collect()\n\n#training lightgbm ranker ---new\n#added valid week sort and gruup\nfrom lightgbm.sklearn import LGBMRanker\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    max_depth=7,\n    n_estimators=300,\n    importance_type='gain',\n    verbose=10\n)\n\n# trainfin.sort_values(['customer_id', 'valid_week'], inplace = True)\n# train_baskets = trainfin.groupby(['customer_id', 'valid_week'])['article_id'].count().values\n\ntrainfin.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets = trainfin.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\nranker = ranker.fit(\n    trainfin.drop(columns = [ 'customer_id', 'article_id', 'label']),\n    trainfin.pop('label'),\n    group=train_baskets,\n)\n#-----------------------------------old 1st part\n\n#old with 3 stgy  and all featres\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\n# test_week = 0\n# trans, test = weeksplit(trans, test_week)\n\nrecalls = []\nfor valid_week in [0, 1, 2]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host \n    CH = CustomerHistory2(trans, 4, 15, valid_week)\n    negdf1 = CH.retrieve(custlist)\n    scaler = MinMaxScaler()\n#     negdf1['score'] = scaler.fit_transform(negdf1.score.values_host.reshape(-1, 1)).flatten()\n    negdf1['strtgy1'] = np.int8(1)\n    \n    BA1 = basket_analysis(trans) #send all data include all weeks except test\n    negdf2 = BA1.retrieve(negdf1[['customer_id','article_id']], t3)\n    negdf2['strtgy2'] = np.int8(1)\n    \n    ts = TopSales(trans, n_weeks=1, n_articles=30, valid_week=valid_week)\n    negdf3 = ts.retrieve(custlist)\n    negdf3['strtgy3'] = np.int8(1)\n    negdf3['score'] = np.int8(0)\n    \n    negdf = cudf.concat([negdf1, negdf2, negdf3]).fillna(0)\n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='outer').fillna({'label':0})\n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)\n\n#added valid score\ntrainfin = cudf.concat(recalls)\ntrainfin = trainfin[trainfin['score'].notna()] #only recalls no positive from weekly valid\n\ntrainfin = (trainfin\n     .merge(customers, on='customer_id')\n     .merge(articles, on='article_id')\n)\n\ntrainfin = trainfin.to_pandas() #check if cuold avoid\ngc.collect()\n\n#training lightgbm ranker ---new\n#added valid week sort and gruup\nfrom lightgbm.sklearn import LGBMRanker\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    max_depth=7,\n    n_estimators=300,\n    importance_type='gain',\n    verbose=10\n)\n\n# trainfin.sort_values(['customer_id', 'valid_week'], inplace = True)\n# train_baskets = trainfin.groupby(['customer_id', 'valid_week'])['article_id'].count().values\n\ntrainfin.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets = trainfin.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\nranker = ranker.fit(\n    trainfin.drop(columns = [ 'customer_id', 'article_id', 'label']),\n    trainfin.pop('label'),\n    group=train_baskets,\n)\n#end old 1\n\n#old with 3 stgy  and seperated score and 'left' join\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\n# test_week = 0\n# trans, test = weeksplit(trans, test_week)\n\nrecalls = []\nfor valid_week in [0, 1, 2]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host \n    CH = CustomerHistory2(trans, 4, 15, valid_week)\n    negdf1 = CH.retrieve(custlist)\n    negdf1 = negdf1.rename(columns={\"score\": \"st1_score\"})\n#     scaler = MinMaxScaler()\n#     negdf1['score'] = scaler.fit_transform(negdf1.score.values_host.reshape(-1, 1)).flatten()\n    negdf1['strtgy1'] = np.int8(1)\n    \n    BA1 = basket_analysis(trans) #send all data include all weeks except test\n    negdf2 = BA1.retrieve(negdf1[['customer_id','article_id']], t3)\n    negdf2 = negdf2.rename(columns={\"score\": \"st2_score\"})\n    negdf2['strtgy2'] = np.int8(1)\n    \n    ts = TopSales2(trans, n_weeks=1, n_articles=30, valid_week=valid_week)\n    negdf3 = ts.retrieve(custlist)\n#     negdf3['strtgy3'] = np.int8(1)\n#     negdf3['score'] = np.int8(0)\n    \n    negdf = cudf.concat([negdf1, negdf2, negdf3]).fillna(0)\n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='left').fillna({'label':0})\n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)\n\n#added valid score\ntrainfin = cudf.concat(recalls)\n# trainfin = trainfin[trainfin['score'].notna()] #only recalls no positive from weekly valid\n\ntrainfin = (trainfin\n     .merge(customers, on='customer_id')\n     .merge(articles, on='article_id')\n)\n\ntrainfin = trainfin.to_pandas() #check if cuold avoid\ngc.collect()\n\n#training lightgbm ranker ---new\n#added valid week sort and gruup\nfrom lightgbm.sklearn import LGBMRanker\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    max_depth=7,\n    n_estimators=300,\n    importance_type='gain',\n    verbose=10\n)\n\n# trainfin.sort_values(['customer_id', 'valid_week'], inplace = True)\n# train_baskets = trainfin.groupby(['customer_id', 'valid_week'])['article_id'].count().values\n\ntrainfin.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets = trainfin.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\nranker = ranker.fit(\n    trainfin.drop(columns = [ 'customer_id', 'article_id', 'label']),\n    trainfin.pop('label'),\n    group=train_baskets,\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:38:27.868803Z","iopub.execute_input":"2022-12-08T05:38:27.869481Z","iopub.status.idle":"2022-12-08T05:49:42.347514Z","shell.execute_reply.started":"2022-12-08T05:38:27.869431Z","shell.execute_reply":"2022-12-08T05:49:42.346849Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#winner -final\nt4 = cudf.read_parquet(path_reduce/'t0sub.pqt') #'t0sub.pqt' -for test, and t0cv.pqt-for internal cv\nt5 = cudf.read_parquet(path_reduce/'t1sub.pqt') #'t1sub.pqt' -for test, and t1cv.pqt-for internal cv\n\n#move it up? we have thi func in cv also\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\nrecalls = []\nfor valid_week in [0, 1, 2]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host \n    CH = CustomerHistory2(trans, 4, 15, valid_week)\n    negdf1 = CH.retrieve(custlist)\n\n    BA1 = basket_analysis(trans) \n    negdf2 = BA1.retrieve(negdf1[['customer_id','article_id']], t4) \n    negdf5 = BA1.retrieve(negdf1[['customer_id','article_id']], t5)\n    negdf2 = cudf.concat([negdf2, negdf5])\n    negdf2 = negdf2.drop_duplicates(['customer_id', 'article_id'])\n    \n    ts = TopSales2(trans, n_weeks=1, n_articles=30, valid_week=valid_week)\n    negdf3 = ts.retrieve(custlist)\n    \n    ts2 = TopSales2(trans, n_weeks=10, n_articles=10, valid_week=valid_week+1) #valid week we take in ts\n    negdf7 = ts2.retrieve(custlist)\n    \n    tag = TopAgeGroup(trans, customers, n_weeks=1, n_articles=15, valid_week=valid_week)\n    negdf4 = tag.retrieve(custlist) \n  \n    negdf = negdf1.copy()\n    for neg in [negdf2, negdf3, negdf4, negdf7]: \n        negdf = negdf.merge(neg, on=['customer_id', 'article_id'], how='outer')\n    negdf = negdf.fillna(0)\n    \n    strgy_names = [x for x in negdf.columns if 'score' not in x][2:]\n    negdf['strgy_count'] = negdf[strgy_names].sum(axis=1).astype('int8')\n    \n    negdf = negdf.drop_duplicates() #probably no need\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = np.int8(1)\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='left').fillna({'label':0})\n    week_recall['valid_week'] = np.int8(valid_week)\n    recalls.append(week_recall)\n\ntrainfin = cudf.concat(recalls)\n\nprint(trainfin.label.value_counts(dropna=False))\n\ntrainfin = (trainfin\n     .merge(customers, on='customer_id')\n     .merge(articles2f, on='article_id') #add features\n)\n\ntrainfin = trainfin.to_pandas() #check if cuold avoid\ngc.collect()\n\nfrom lightgbm.sklearn import LGBMRanker #moveup\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    max_depth=7,\n    n_estimators=300,\n    importance_type='gain',\n    verbose=10\n)\n\n# trainfin.sort_values(['customer_id', 'valid_week'], inplace = True)\n# train_baskets = trainfin.groupby(['customer_id', 'valid_week'])['article_id'].count().values\n\ntrainfin.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets = trainfin.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\nranker = ranker.fit(\n    trainfin.drop(columns = [ 'customer_id','article_id', 'label']), \n    trainfin.pop('label'),\n    group=train_baskets,\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T09:01:13.594056Z","iopub.execute_input":"2023-05-25T09:01:13.594408Z","iopub.status.idle":"2023-05-25T09:49:46.563830Z","shell.execute_reply.started":"2023-05-25T09:01:13.594370Z","shell.execute_reply":"2023-05-25T09:49:46.563044Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n  \"When using a sequence of booleans for `ascending`, \"\n","output_type":"stream"},{"name":"stdout","text":"0    9535587\n1      52277\nName: label, dtype: int32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.828266\n[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.328501\n[LightGBM] [Debug] init for col-wise cost 1.000754 seconds, init for row-wise cost 4.691568 seconds\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.121045 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Debug] Using Sparse Multi-Val Bin\n[LightGBM] [Info] Total Bins 2492\n[LightGBM] [Info] Number of data points in the train set: 9587864, number of used features: 34\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","output_type":"stream"}]},{"cell_type":"code","source":"#old basket score dict--- to delete copy down\n#loading data to predict\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\nsample_sub['cusid'] = sample_sub.customer_id.map(DHC.mapdict['customer_id_to_idx_dict'])\n\n#generate candidates\nCH2 = CustomerHistory(trans, 4, 15, -1) #veify\ncustlist2 = sample_sub.cusid.unique().values_host\ncandidates1 = CH2.retrieve(custlist2)\ncandidates1['score'] = scaler.fit_transform(candidates1.score.values_host.reshape(-1, 1)).flatten()\ncandidates1['strtgy1'] = np.int8(1)\ncandidates2 = candidates1.copy()\ncandidates2 = candidates2.loc[candidates2.score.values>0]    \n\ncandidates2['score'] = candidates2.article_id.map(new_score) #change to real acore check if its really taht low\ncandidates2['article_id'] = candidates2.article_id.map(new_pairs)\n\n\n# candidates2['article_id'] = candidates2.article_id.map(new_pairs)\n# candidates2['score'] = candidates2.article_id.map(new_score)\n\ncandidates2['strtgy2'] = np.int8(1)\n\n\ncandidates = cudf.concat([candidates1, candidates2]).fillna(0)\ncandidates['valid_week'] = -1 #check , and also for np.int8(-1)\n\ndel trans\ngc.collect()\n\n\n#add features and addjusting to model\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n\n#old manual adjusting names----------------------------------------------------------------------------------\n#loading data to predict\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\n# sample_sub['cusid'] = sample_sub.customer_id.map(DHC.mapdict['customer_id_to_idx_dict'])\nsample_sub['cusid'] = sample_sub.customer_id.map(customer_id_to_idx_dict)\n\n#generate candidates\ncustlist2 = sample_sub.cusid.unique().values_host\nCH2 = CustomerHistory2(trans, 4, 15, -1) #veify\ncandidates1 = CH2.retrieve(custlist2)\n# candidates1['score'] = scaler.fit_transform(candidates1.score.values_host.reshape(-1, 1)).flatten()\ncandidates1['strtgy1'] = np.int8(1)\n\nBAv1 = basket_analysis(trans) #2nd time calculte the pair, need to add loading option\ncandidates2 = BAv1.retrieve(candidates1[['customer_id','article_id']], t3)\ncandidates2['strtgy2'] = np.int8(1)\n\nts1 = TopSales(trans, n_weeks=1, n_articles=30, valid_week=-1)\ncandidates3 = ts1.retrieve(custlist2)\ncandidates3['strtgy3'] = np.int8(1)\ncandidates3['score'] = np.int8(0)\n\ncandidates = cudf.concat([candidates1, candidates2, candidates3]).fillna(0)\ncandidates['valid_week'] = -1 #check , and also for np.int8(-1)\n\ndel trans\ngc.collect()\n\n\n#add features and addjusting to model\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n\n#old before adding candiate4----------------------------------------------------------------------------\n#old final with seperated scores\n#loading data to predict\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\n# sample_sub['cusid'] = sample_sub.customer_id.map(DHC.mapdict['customer_id_to_idx_dict'])\nsample_sub['cusid'] = sample_sub.customer_id.map(customer_id_to_idx_dict)\n\n#generate candidates\ncustlist2 = sample_sub.cusid.unique().values_host\nCH2 = CustomerHistory2(trans, 4, 15, -1) #veify\ncandidates1 = CH2.retrieve(custlist2)\n\nBAv1 = basket_analysis(trans) #2nd time calculte the pair, need to add loading option\ncandidates2 = BAv1.retrieve(candidates1[['customer_id','article_id']], t4)\n# candidates4 = BAv1.retrieve(candidates1[['customer_id','article_id']], t5)\n# candidates4 = candidates4.rename(columns={\"score_basket_rank0\": \"score_basket_rank1\", \"basket_rank0\": \"basket_rank1\" })\n\nts1 = TopSales2(trans, n_weeks=1, n_articles=30, valid_week=-1)\ncandidates3 = ts1.retrieve(custlist2)\n\n# candidates = cudf.concat([candidates1, candidates2, candidates3]).fillna(0)\ncandidates = candidates1.copy()\nfor can in [candidates2, candidates3]:#, candidates4]:\n    candidates = candidates.merge(can, on=['customer_id', 'article_id'], how='outer')\ncandidates = candidates.fillna(0)\n\ncandidates['valid_week'] = -1 #check , and also for np.int8(-1)\n\ndel trans\ngc.collect()\n\n\n#add features and addjusting to model\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n#----------------------------------------------------------------------\n","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:49:42.351603Z","iopub.execute_input":"2022-12-08T05:49:42.353253Z","iopub.status.idle":"2022-12-08T05:50:22.286758Z","shell.execute_reply.started":"2022-12-08T05:49:42.353218Z","shell.execute_reply":"2022-12-08T05:50:22.285967Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating the 'candidates4' file to save memory - future will be by name and from the class\n#till finilize of age strgy with file (also need un-# the customer load and transfer down)\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\nsample_sub['cusid'] = sample_sub.customer_id.map(customer_id_to_idx_dict)\ncustlist2 = sample_sub.cusid.unique().values_host\ntagv1 = TopAgeGroup(trans, customers, n_weeks=1, n_articles=15, valid_week=-1)\ncandidates4 = tagv1.retrieve(custlist2)\ncandidates4['score_top_age_group1w_15a'] = candidates4['score_top_age_group1w_15a'].astype('int16')\ncandidates4.to_csv('candidates4sub.csv', chunksize=1_000_000) ","metadata":{"execution":{"iopub.status.busy":"2023-03-13T02:33:11.701427Z","iopub.execute_input":"2023-03-13T02:33:11.701794Z","iopub.status.idle":"2023-03-13T02:33:43.970046Z","shell.execute_reply.started":"2023-03-13T02:33:11.701755Z","shell.execute_reply":"2023-03-13T02:33:43.969215Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gc.collect()\ndel trainfin\ndel recalls\ndel negdf\ndel negdf1\ndel negdf2\ndel negdf3\ndel negdf4\ndel negdf5\ndel negdf7\n# gc.collect()\n\ndel train, valid, week_recall #can move to before model training\n# gc.collect()\n\ndel CH, BA1, ts, ts2, tag\ngc.collect()\n\n#winner - Final\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\n# sample_sub['cusid'] = sample_sub.customer_id.map(DHC.mapdict['customer_id_to_idx_dict'])\nsample_sub['cusid'] = sample_sub.customer_id.map(customer_id_to_idx_dict)\nsample_sub['cusid'] = sample_sub['cusid'].astype('int32')\n\n#generate candidates\ncustlist2 = sample_sub.cusid.unique().values_host\n\nCH2 = CustomerHistory2(trans, 4, 15, -1) \ncandidates1 = CH2.retrieve(custlist2)\n\nBAv1 = basket_analysis(trans) #2nd time calculte the pair, need to add loading option\ncandidates2 = BAv1.retrieve(candidates1[['customer_id','article_id']], t4)\ncandidates5 = BAv1.retrieve(candidates1[['customer_id','article_id']], t5) \ncandidates2 = cudf.concat([candidates2, candidates5])\ncandidates2 = candidates2.drop_duplicates(['customer_id', 'article_id'])\n\nts1 = TopSales2(trans, n_weeks=1, n_articles=30, valid_week=-1)\ncandidates3 = ts1.retrieve(custlist2)\n\nts2 = TopSales2(trans, n_weeks=10, n_articles=10, valid_week=0) #valid_week=0 maybe better cause the upper one alreadt taking last week\ncandidates7 = ts2.retrieve(custlist2)\n\n#loading and adjusting candidtes4 to save memory (file was strgy with same parameters)\ncandidates4 = cudf.read_csv(path_reduce/'candidates4sub.csv', index_col=0)\ncandidates4['article_id'] = candidates4['article_id'].astype('int32')\ncandidates4['customer_id'] = candidates4['customer_id'].astype('int32') #----new to solve for all cands\ncandidates4['score_top_age_group1w_15a'] = candidates4['score_top_age_group1w_15a'].astype('int16')\ncandidates4['top_age_group1w_15a'] = candidates4['top_age_group1w_15a'].astype('int8')\n# tagv1 = TopAgeGroup(trans, customers, n_weeks=1, n_articles=15, valid_week=-1)\n# candidates4 = tagv1.retrieve(custlist2)\n\ncandidates = candidates1.copy()\nfor can in [candidates2, candidates3, candidates4, candidates7]:\n    candidates = candidates.merge(can, on=['customer_id', 'article_id'], how='outer')\ncandidates = candidates.fillna(0)\n\nstrgy_names = [x for x in candidates.columns if 'score' not in x][2:] #prbly can cancel\ncandidates['strgy_count'] = candidates[strgy_names].sum(axis=1).astype('int8')\n\ncandidates['valid_week'] = np.int8(-1) \n\ndel trans\ndel candidates1\ndel candidates2\ndel candidates3\ndel candidates4\ndel candidates5\ndel candidates7\ndel custlist2\ndel custlist\ndel t4\ndel t5\n# gc.collect()\n\ndel CH2, BAv1, ts1, ts2\ngc.collect()\n\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n)\ndel customers\n# gc.collect()\n\ncandidates =(\n    candidates\n    .merge(articles2f, on = ('article_id'))\n)\ndel articles2f\n\n#winner \npreds = []\nbatch_size = 1_000_000\nfor bucket in tqdm(range(0, len(candidates), batch_size)):\n  outputs = ranker.predict(\n      candidates.iloc[bucket: bucket+batch_size]\n      .drop(columns = ['customer_id', 'article_id'])\n      .to_pandas()\n      )\n  preds.append(outputs)\n\npreds = np.concatenate(preds)\ncandidates['preds'] = preds\npreds = candidates[['customer_id', 'article_id', 'preds']]\n\ndel candidates\ngc.collect()\n\n# tolist and map(func) not working well with cudf - probably can chnage the solution sort, cut top 12, convert, list to make it much faster and with cudf\npreds = preds.to_pandas()\n#adjusting format to top 12\npreds.sort_values(['customer_id', 'preds'], ascending=False, inplace = True) \npreds = ( \n    preds\n    .groupby('customer_id')[['article_id']]\n    .aggregate(lambda x: x.tolist())\n)\n\nsample_sub = sample_sub.to_pandas()\nsample_sub = sample_sub.merge(preds, left_on='cusid', right_index=True, how='left')\n\ndef forcon(pr):\n    forc = []\n    for i in pr:\n#         forc.append('0' + str(DHA.mapdict['idx_to_article_id_dict'][i]))\n        forc.append('0' + str(idx_to_article_id_dict[i]))\n    return ' '.join(forc)\n\nsample_sub['prediction2'] = sample_sub.article_id.map(forcon)\n\nsample_sub = sample_sub.drop(columns=['prediction', 'cusid' ,'article_id'])\nsample_sub = sample_sub.rename(columns={'prediction2': 'prediction'})\nsample_sub['prediction'] = sample_sub['prediction'].apply(lambda x: x[:131])\nsample_sub.to_csv('f5stgyf.csv.gz', index=False)\n# !kaggle competitions submit -c h-and-m-personalized-fashion-recommendations -f 'data/subs/{sub_name}.csv.gz' -m {sub_name}","metadata":{"execution":{"iopub.status.busy":"2023-05-25T09:49:46.565732Z","iopub.execute_input":"2023-05-25T09:49:46.566240Z","iopub.status.idle":"2023-05-25T10:05:21.801773Z","shell.execute_reply.started":"2023-05-25T09:49:46.566191Z","shell.execute_reply":"2023-05-25T10:05:21.800946Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 57/57 [13:20<00:00, 14.04s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"#old basket score dict\n#loading data to predict\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\nsample_sub['cusid'] = sample_sub.customer_id.map(DHC.mapdict['customer_id_to_idx_dict'])\n\n#generate candidates\nCH2 = CustomerHistory(trans, 4, 15, -1) #veify\ncustlist2 = sample_sub.cusid.unique().values_host\ncandidates1 = CH2.retrieve(custlist2)\ncandidates1['score'] = scaler.fit_transform(candidates1.score.values_host.reshape(-1, 1)).flatten()\ncandidates1['strtgy1'] = np.int8(1)\ncandidates2 = candidates1.copy()\ncandidates2 = candidates2.loc[candidates2.score.values>0]    \n\ncandidates2['score'] = candidates2.article_id.map(new_score) #change to real acore check if its really taht low\ncandidates2['article_id'] = candidates2.article_id.map(new_pairs)\n\n\n# candidates2['article_id'] = candidates2.article_id.map(new_pairs)\n# candidates2['score'] = candidates2.article_id.map(new_score)\n\ncandidates2['strtgy2'] = np.int8(1)\n\n\ncandidates = cudf.concat([candidates1, candidates2]).fillna(0)\ncandidates['valid_week'] = -1 #check , and also for np.int8(-1)\n\ndel trans\ngc.collect()\n\n\n#add features and addjusting to model\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n\n#old manual adjusting names----------------------------------------------------------------------------------\n#loading data to predict\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\n# sample_sub['cusid'] = sample_sub.customer_id.map(DHC.mapdict['customer_id_to_idx_dict'])\nsample_sub['cusid'] = sample_sub.customer_id.map(customer_id_to_idx_dict)\n\n#generate candidates\ncustlist2 = sample_sub.cusid.unique().values_host\nCH2 = CustomerHistory2(trans, 4, 15, -1) #veify\ncandidates1 = CH2.retrieve(custlist2)\n# candidates1['score'] = scaler.fit_transform(candidates1.score.values_host.reshape(-1, 1)).flatten()\ncandidates1['strtgy1'] = np.int8(1)\n\nBAv1 = basket_analysis(trans) #2nd time calculte the pair, need to add loading option\ncandidates2 = BAv1.retrieve(candidates1[['customer_id','article_id']], t3)\ncandidates2['strtgy2'] = np.int8(1)\n\nts1 = TopSales(trans, n_weeks=1, n_articles=30, valid_week=-1)\ncandidates3 = ts1.retrieve(custlist2)\ncandidates3['strtgy3'] = np.int8(1)\ncandidates3['score'] = np.int8(0)\n\ncandidates = cudf.concat([candidates1, candidates2, candidates3]).fillna(0)\ncandidates['valid_week'] = -1 #check , and also for np.int8(-1)\n\ndel trans\ngc.collect()\n\n\n#add features and addjusting to model\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n\n#old before adding candiate4----------------------------------------------------------------------------\n#old final with seperated scores\n#loading data to predict\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\n# sample_sub['cusid'] = sample_sub.customer_id.map(DHC.mapdict['customer_id_to_idx_dict'])\nsample_sub['cusid'] = sample_sub.customer_id.map(customer_id_to_idx_dict)\n\n#generate candidates\ncustlist2 = sample_sub.cusid.unique().values_host\nCH2 = CustomerHistory2(trans, 4, 15, -1) #veify\ncandidates1 = CH2.retrieve(custlist2)\n\nBAv1 = basket_analysis(trans) #2nd time calculte the pair, need to add loading option\ncandidates2 = BAv1.retrieve(candidates1[['customer_id','article_id']], t4)\n# candidates4 = BAv1.retrieve(candidates1[['customer_id','article_id']], t5)\n# candidates4 = candidates4.rename(columns={\"score_basket_rank0\": \"score_basket_rank1\", \"basket_rank0\": \"basket_rank1\" })\n\nts1 = TopSales2(trans, n_weeks=1, n_articles=30, valid_week=-1)\ncandidates3 = ts1.retrieve(custlist2)\n\n# candidates = cudf.concat([candidates1, candidates2, candidates3]).fillna(0)\ncandidates = candidates1.copy()\nfor can in [candidates2, candidates3]:#, candidates4]:\n    candidates = candidates.merge(can, on=['customer_id', 'article_id'], how='outer')\ncandidates = candidates.fillna(0)\n\ncandidates['valid_week'] = -1 #check , and also for np.int8(-1)\n\ndel trans\ngc.collect()\n\n\n#add features and addjusting to model\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n#----------------------------------------------------------------------\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#old 1 strgy\ndef weeksplit(df, week_number=0):\n    valid = df.loc[df.week==week_number]\n    train = df.loc[df.week>week_number]\n    return train, valid\n\n# test_week = 0\n# trans, test = weeksplit(trans, test_week)\n\nrecalls = []\nfor valid_week in [0, 1, 2]:\n    train, valid = weeksplit(trans, valid_week)\n    custlist = valid.customer_id.unique().values_host #maybe take also for valid?\n    CH = CustomerHistory(trans, 4, 15, valid_week)\n    negdf = CH.retrieve(custlist)\n    negdf = negdf.drop_duplicates()\n    buys = valid[['customer_id', 'article_id']].drop_duplicates()\n    buys['label'] = 1\n    week_recall = negdf.merge(buys, on=['customer_id', 'article_id'], how='outer').fillna({'label':0})\n    week_recall['valid_week'] = valid_week\n    recalls.append(week_recall)\n\n#added valid score\ntrainfin = cudf.concat(recalls)\ntrainfin = trainfin[trainfin['score'].notna()] #only recalls no positive from weekly valid\n\n# trainfin = trainfin.fillna({'score':-1})\n\ntrainfin = (trainfin\n     .merge(customers, on='customer_id')\n     .merge(articles, on='article_id')\n)\n\ntrainfin =  trainfin[['valid_week', 'customer_id', 'article_id','FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age','product_type_name', 'product_group_name',\n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name', 'perceived_colour_master_name',\n       'department_name', 'index_name', 'index_group_name', 'section_name',\n       'garment_group_name','label', 'score']]\n\ntrainfin = trainfin.to_pandas() #check if cuold avoid\ngc.collect()\n\n#training lightgbm ranker ---new\n#added valid week sort and gruup\nfrom lightgbm.sklearn import LGBMRanker\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    max_depth=7,\n    n_estimators=300,\n    importance_type='gain',\n    verbose=10\n)\n\n# trainfin.sort_values(['customer_id', 'valid_week'], inplace = True)\n# train_baskets = trainfin.groupby(['customer_id', 'valid_week'])['article_id'].count().values\n\ntrainfin.sort_values(['valid_week', 'customer_id'], inplace = True)\ntrain_baskets = trainfin.groupby(['valid_week', 'customer_id'])['article_id'].count().values\n\nranker = ranker.fit(\n    trainfin.drop(columns = [ 'customer_id', 'article_id', 'label']),\n    trainfin.pop('label'),\n    group=train_baskets,\n)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#old 1 stgy part 2\n#loading data to predict\nsample_sub = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\nsample_sub['cusid'] = sample_sub.customer_id.map(DHC.mapdict['customer_id_to_idx_dict'])\n\n#generate candidates\nCH2 = CustomerHistory(trans, 4, 12, -1) #veify\ncustlist2 = sample_sub.cusid.unique().values_host\ncandidates = CH2.retrieve(custlist2)\ncandidates['valid_week'] = -1 #check\n\ndel trans\ngc.collect()\n\n\n#add features and addjusting to model\ncandidates = (\n    candidates\n    .merge(customers, on = ('customer_id'))\n    .merge(articles, on = ('article_id'))\n)\n\n\ncandidates =  candidates[['valid_week', 'customer_id', 'article_id','FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age','product_type_name', 'product_group_name',\n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name', 'perceived_colour_master_name',\n       'department_name', 'index_name', 'index_group_name', 'section_name',\n       'garment_group_name','score']]\n\n#predicting\n\ncandidates = candidates.to_pandas()\n\npreds = []\nbatch_size = 1_000_000\nfor bucket in tqdm(range(0, len(candidates), batch_size)):\n  outputs = ranker.predict(\n      candidates.iloc[bucket: bucket+batch_size]\n      .drop(columns = ['customer_id', 'article_id'])\n      )\n  preds.append(outputs)\n    \npreds = np.concatenate(preds)\ncandidates['preds'] = preds\npreds = candidates[['customer_id', 'article_id', 'preds']]\n\ndel candidates\ngc.collect()\n\n#adjusting format to top 12\npreds.sort_values(['customer_id', 'preds'], ascending=False, inplace = True)\npreds = (\n    preds\n    .groupby('customer_id')[['article_id']]\n    .aggregate(lambda x: x.tolist())\n)\n\nsample_sub = sample_sub.to_pandas()\n\nsample_sub = sample_sub.merge(preds, left_on='cusid', right_index=True, how='left')\n\ngc.collect()\n\ndef forcon(pr):\n    forc = []\n    for i in pr:\n        forc.append('0' + str(DHA.mapdict['idx_to_article_id_dict'][i]))\n    return ' '.join(forc)\n\nsample_sub['prediction2'] = sample_sub.article_id.map(forcon)\nsample_sub = sample_sub.drop(columns=['prediction', 'cusid' ,'article_id'])\nsample_sub = sample_sub.rename(columns={'prediction2': 'prediction'})\nsample_sub.to_csv('fmodelnewpro55w.csv.gz', index=False)\n# !kaggle competitions submit -c h-and-m-personalized-fashion-recommendations -f 'data/subs/{sub_name}.csv.gz' -m {sub_name}\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare validation for submission format","metadata":{}},{"cell_type":"code","source":"#copied from up of the final soultion - not in use for final solution\n#########currently not in use\n# import matplotlib.pyplot as plt\nimport warnings #to remove warning (long time wanted to see )\n# warnings.filterwarnings('ignore') check if this remove the warning\n# import seaborn as sns\n# from PIL import Image (in chris basket there is some discussio nabout loaidng images better)\ntqdm.pandas()\nprint(f\"pandas version: {pd.__version__}\")\n# pd.options.display.max_colwidth = 200\nimport pdb\npdb.set_trace()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looks like submittion of older solution \n#Series with map customer and what he bought in the valid week\n# actual = valid.to_pandas().groupby('customer_id').article_id.apply(list)\n#convert to string of articles like the submission format\ndef to_sub(row):\n    s=''\n    for l in row:\n        s=s+'0'+ str(l) + ' '\n    return s.strip()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T04:59:05.993057Z","iopub.execute_input":"2022-05-27T04:59:05.993357Z","iopub.status.idle":"2022-05-27T04:59:05.998934Z","shell.execute_reply.started":"2022-05-27T04:59:05.993325Z","shell.execute_reply":"2022-05-27T04:59:05.997982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#same us the upper one only for first fold (validf1)\nactual = validf1.to_pandas().groupby('customer_id').article_id.apply(list)\nactualsub = actual.apply(to_sub) ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:20:58.270034Z","iopub.execute_input":"2022-05-27T05:20:58.270632Z","iopub.status.idle":"2022-05-27T05:21:00.016323Z","shell.execute_reply.started":"2022-05-27T05:20:58.270588Z","shell.execute_reply":"2022-05-27T05:21:00.015485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#same us the upper one only for 2nd fold (validf2)\nactual = validf2.to_pandas().groupby('customer_id').article_id.apply(list)\nactualsub = actual.apply(to_sub) ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:16:20.36648Z","iopub.execute_input":"2022-05-27T05:16:20.366791Z","iopub.status.idle":"2022-05-27T05:16:21.991033Z","shell.execute_reply.started":"2022-05-27T05:16:20.366752Z","shell.execute_reply":"2022-05-27T05:16:21.990206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#same us the upper one only for 3rd fold (validf3)\nactual = validf3.to_pandas().groupby('customer_id').article_id.apply(list)\nactualsub = actual.apply(to_sub) ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:09.676081Z","iopub.execute_input":"2022-05-27T05:19:09.676363Z","iopub.status.idle":"2022-05-27T05:19:11.383897Z","shell.execute_reply.started":"2022-05-27T05:19:09.676332Z","shell.execute_reply":"2022-05-27T05:19:11.383063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check idea with internal cv","metadata":{}},{"cell_type":"code","source":"med_big, mean_big, friq_big  = top_weekly(trainf3)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T11:34:13.595863Z","iopub.execute_input":"2022-05-16T11:34:13.596112Z","iopub.status.idle":"2022-05-16T11:34:23.963609Z","shell.execute_reply.started":"2022-05-16T11:34:13.596083Z","shell.execute_reply":"2022-05-16T11:34:23.962872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_cv(submit, big, valid):\n    top12 = '0' + big[:12].index.to_pandas().astype('str') \n    top12 = ' '.join(top12.to_list()) \n    submit['prediction'] = top12\n    submit['customer_id'] = submit.customer_id.map(id_to_index_dict)\n    submitmk = submit.to_pandas().merge(valid, left_on='customer_id', right_index=True)\n    return mapk(valid.str.split(' '), submitmk.prediction.str.split(' '))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T11:21:30.620891Z","iopub.execute_input":"2022-05-16T11:21:30.621163Z","iopub.status.idle":"2022-05-16T11:21:30.626768Z","shell.execute_reply.started":"2022-05-16T11:21:30.621135Z","shell.execute_reply":"2022-05-16T11:21:30.626037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\nto_cv(submit, med_big, actualsub)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### solution2 - run weekly stats on seperate gender","metadata":{}},{"cell_type":"code","source":"def by_gen(df, artiscore):\n    df = df.merge(artiscore, on='article_id')\n    ###can do one time transfer\n    custsex = df.groupby('customer_id').index_group_name.sum()\n    custsex = custsex>0\n    df = df.merge(custsex, left_on='customer_id', right_index=True)\n    df.drop(columns=['index_group_name_x'], inplace=True)\n    ###can do one time transfer#########\n    df.rename({'index_group_name_y': 'is_female'}, axis=1, inplace=True)\n    df['week_n'] = df.t_dat.to_pandas().dt.isocalendar().week\n    df['year'] = df.t_dat.dt.year\n    sold_per_week = df.groupby(['is_female', 'year', 'week_n', 'article_id']).sales_channel_id.count().reset_index() \n    sold_per_week.rename({'sales_channel_id': 'Total_sold'}, axis=1, inplace=True)\n    return sold_per_week","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:14:24.626358Z","iopub.execute_input":"2022-05-27T05:14:24.626652Z","iopub.status.idle":"2022-05-27T05:14:24.634663Z","shell.execute_reply.started":"2022-05-27T05:14:24.626619Z","shell.execute_reply":"2022-05-27T05:14:24.633737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sold_per_week = by_gen(trainf1, articles)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:21:10.827695Z","iopub.execute_input":"2022-05-27T05:21:10.828424Z","iopub.status.idle":"2022-05-27T05:21:23.058257Z","shell.execute_reply.started":"2022-05-27T05:21:10.828386Z","shell.execute_reply":"2022-05-27T05:21:23.057425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top12f = sold_per_week.loc[sold_per_week.is_female].groupby('article_id').Total_sold.mean().nlargest(12)\ntop12f = '0' + top12f[:12].index.to_pandas().astype('str') \ntop12f = ' '.join(top12f.to_list())\n\ntop12m = sold_per_week.loc[~sold_per_week.is_female].groupby('article_id').Total_sold.mean().nlargest(12)\ntop12m = '0' + top12m[:12].index.to_pandas().astype('str') \ntop12m = ' '.join(top12m.to_list())","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:21:28.765079Z","iopub.execute_input":"2022-05-27T05:21:28.765379Z","iopub.status.idle":"2022-05-27T05:21:28.815225Z","shell.execute_reply.started":"2022-05-27T05:21:28.765346Z","shell.execute_reply":"2022-05-27T05:21:28.81437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\nsubmit2 = submit.merge(custsex, how='left', left_index=True, right_index=True)\nsubmit2.index_group_name.fillna(True, inplace=True)\nsubmit2.loc[submit2.index_group_name, 'prediction'] = top12f\nsubmit2.loc[~submit2.index_group_name, 'prediction'] = top12m\nsubmit2.drop(columns=['index_group_name'], inplace=True)\nsubmit2 = submit2.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:21:32.876086Z","iopub.execute_input":"2022-05-27T05:21:32.87638Z","iopub.status.idle":"2022-05-27T05:21:33.268134Z","shell.execute_reply.started":"2022-05-27T05:21:32.876346Z","shell.execute_reply":"2022-05-27T05:21:33.267267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#do function so can use the submit2 to LB\nsubmit2['customer_id'] = submit2.customer_id.map(id_to_index_dict)\nsubmitmk = submit2.merge(actualsub, left_on='customer_id', right_index=True)\nmapk(actualsub.str.split(' '), submitmk.prediction.str.split(' '))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:21:36.598439Z","iopub.execute_input":"2022-05-27T05:21:36.598745Z","iopub.status.idle":"2022-05-27T05:21:40.894916Z","shell.execute_reply.started":"2022-05-27T05:21:36.598709Z","shell.execute_reply":"2022-05-27T05:21:40.89415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  3 - most popular only in the last week *****need validate","metadata":{}},{"cell_type":"code","source":"submit2 = cudf.read_csv(path/'sample_submission.csv', low_memory=False)\nday = np.timedelta64(1,'D')\nw1 = train.loc[train.t_dat>train.t_dat.max() - 7*day]\ntop12w1 = w1.article_id.value_counts()[:12].index\ntop12w1 = '0' + top12w1.astype('str') \ntop12w1 = ' '.join(top12w1.to_pandas().to_list()) \nsubmit2['prediction'] = top12w1","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:31:35.916828Z","iopub.execute_input":"2022-06-15T07:31:35.917208Z","iopub.status.idle":"2022-06-15T07:31:36.238576Z","shell.execute_reply.started":"2022-06-15T07:31:35.917168Z","shell.execute_reply":"2022-06-15T07:31:36.237831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check result in LB","metadata":{}},{"cell_type":"markdown","source":"#### solution1","metadata":{}},{"cell_type":"code","source":"#all the train, no splits, same pipeline\nmed_big, mean_big, friq_big  = top_weekly(train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generating file for submission (return the DF)\ndef to_submit(submit, big, name='submit'):\n    top12 = '0' + big[:12].index.to_pandas().astype('str') \n    top12 = ' '.join(top12.to_list()) \n    submit['prediction'] = top12\n    name = name + '.csv'\n    submit.to_csv(name, index=False) \n    return submit","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### solution 2","metadata":{}},{"cell_type":"code","source":"#offcourse - use the submit2 without running the CV\nsubmit2.to_csv('genderlbl.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# EDA on the images - cool notebook from other competation\nhttps://www.kaggle.com/code/ishandutta/v7-shopee-indepth-eda-one-stop-for-all-your-needs/notebook\n#PCA seasnality anaysis (and plotly.express) but his newbiw not sure how good \nhttps://www.kaggle.com/code/lichtlab/h-m-data-deep-dive-chap-1-understand-article/notebook","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reducing dataset size by random sampling (most plots remain the same with 1000 roes samples... stats rules!!!)\ntoytrain = train.sample(frac=0.001, axis=0 )\ndisplay(toytrain)\ntoytrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:27:28.831615Z","iopub.execute_input":"2022-06-10T06:27:28.831934Z","iopub.status.idle":"2022-06-10T06:27:29.091011Z","shell.execute_reply.started":"2022-06-10T06:27:28.831901Z","shell.execute_reply":"2022-06-10T06:27:29.089509Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fast plot graphs directly from pandas trick\nplt.figure(figsize=(16, 9))\ntoytrain.to_pandas().groupby([\"t_dat\"])[\"article_id\"].count().plot()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:28:13.161155Z","iopub.execute_input":"2022-06-10T06:28:13.161446Z","iopub.status.idle":"2022-06-10T06:28:13.595745Z","shell.execute_reply.started":"2022-06-10T06:28:13.161415Z","shell.execute_reply":"2022-06-10T06:28:13.594838Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nicer formating for pandas tables\ntoyarticles = toyarticles.to_pandas()\ntoyarticles.describe().T.sort_values(by='std' , ascending = False)\\\n.style.background_gradient(cmap='GnBu')\\\n.bar(subset=[\"max\"], color='#F8766D')\\\n.bar(subset=[\"mean\",], color='#00BFC4')\n                    ","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:56:48.787211Z","iopub.execute_input":"2022-06-09T03:56:48.787486Z","iopub.status.idle":"2022-06-09T03:56:48.840885Z","shell.execute_reply.started":"2022-06-09T03:56:48.787456Z","shell.execute_reply":"2022-06-09T03:56:48.840224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom wordcloud import WordCloud #, STOPWORDS internal set used by the finction by default\n\nwords_raw = articles['detail_desc'].dropna().to_pandas().apply(nltk.word_tokenize)\nbag_of_words = \" \".join(words_raw.explode())\n\nfig, ax1 = plt.subplots(figsize=(8,6))\nwordcloud = WordCloud( background_color=\"white\", height=300, contour_width=3).generate(bag_of_words) #stopwords=stopwords\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-07T06:07:34.031116Z","iopub.execute_input":"2022-06-07T06:07:34.031415Z","iopub.status.idle":"2022-06-07T06:08:21.951732Z","shell.execute_reply.started":"2022-06-07T06:07:34.031385Z","shell.execute_reply":"2022-06-07T06:08:21.95108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_rows = None \npd.options.display.max_columns = None\n# pd.set_option('display.float_format', '{:.4f}'.format) \npd.options.display.float_format = '{:.4f}'.format","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build custom display function for multiple images\ntop10 = toytrain.groupby('article_id').sales_channel_id.count().nlargest(10).index\nf, ax = plt.subplots(2, 5, figsize=(20, 10))\nax = ax.flatten() #to plot on more than one row with no double indx\nfor aid, a in zip(top10.to_arrow(), ax)\n    faid = '0' + str(aid)\n    fp = path/'images'/faid[:3]/(faid+'.jpg')\n    if fp.exists():\n        im = Image.open(fp)\n        a.imshow(im)   \n        a.set_title(f'ID: {faid}')\n        a.set_xticks([], [])\n        a.set_yticks([], [])\n        a.grid(False)\n        a.set_xlabel('Nada', fontsize=10)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T05:34:56.228043Z","iopub.execute_input":"2022-06-06T05:34:56.228502Z","iopub.status.idle":"2022-06-06T05:34:59.425102Z","shell.execute_reply.started":"2022-06-06T05:34:56.228463Z","shell.execute_reply":"2022-06-06T05:34:59.424326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.ticker as ticker\n\nf, ax = plt.subplots(figsize=(10, 5))\nax = sns.histplot(toycust.to_pandas(), x='age', bins=toycust.age.nunique(), color='magenta', stat='percent')\nax.yaxis.set_major_formatter(ticker.PercentFormatter())\nmedian = toycust['age'].median()\nax.axvline(x=median, color=\"green\", ls=\"--\")\nax.text(median, 3.5, 'median: {}'.format(round(median,1)), rotation='vertical', ha='right')\nax.text(40, 8, 'Distribution of customers age', color='black', fontsize=10, ha='left', va='bottom', weight='bold', style='italic')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T03:58:43.030847Z","iopub.execute_input":"2022-06-08T03:58:43.031324Z","iopub.status.idle":"2022-06-08T03:58:43.339881Z","shell.execute_reply.started":"2022-06-08T03:58:43.03127Z","shell.execute_reply":"2022-06-08T03:58:43.339131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toytrain['week_n'] = toytrain.t_dat.to_pandas().dt.isocalendar().week\ntoytrain['year'] = toytrain.t_dat.dt.year\nsold_per_week = toytrain.groupby(['year', 'week_n', 'article_id']).sales_channel_id.count().reset_index()\nsold_per_week.rename({'sales_channel_id': 'Total_sold'}, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T10:03:02.481596Z","iopub.execute_input":"2022-06-07T10:03:02.482311Z","iopub.status.idle":"2022-06-07T10:03:02.514437Z","shell.execute_reply.started":"2022-06-07T10:03:02.482273Z","shell.execute_reply":"2022-06-07T10:03:02.513747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generalist = ['0706016001', '0706016002', '0372860001', '0610776002', '0759871002', '0464297007', '0372860002', '0610776001', '0399223001', '0706016003', '0720125001', '0156231001'] \ngl = [int(x) for x in generalist[:3]]\nmainitems = sold_per_week.loc[sold_per_week.article_id.isin(gl)]\nsns.catplot(x=\"week_n\", y=\"Total_sold\", col=\"year\",row = 'article_id',  kind=\"bar\", data=mainitems.to_pandas())","metadata":{"execution":{"iopub.status.busy":"2022-06-07T09:19:07.699496Z","iopub.execute_input":"2022-06-07T09:19:07.699755Z","iopub.status.idle":"2022-06-07T09:19:11.94161Z","shell.execute_reply.started":"2022-06-07T09:19:07.699725Z","shell.execute_reply":"2022-06-07T09:19:11.940877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mbreak(s):\n    return s.to_pandas().sort_values().diff().max().days  \nbreaks = train.groupby('article_id').t_dat.apply(mbreak)\nd=range(30, 720, 30)\nfor i in tqdm(d):\n    print(f'total {(breaks<i).sum()} items {(breaks<i).mean()*100:.2f}% for {i} days')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T05:25:10.345414Z","iopub.execute_input":"2022-06-06T05:25:10.346117Z","iopub.status.idle":"2022-06-06T05:28:06.242089Z","shell.execute_reply.started":"2022-06-06T05:25:10.346071Z","shell.execute_reply":"2022-06-06T05:28:06.241407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f'{breaks.isnull().mean()*100:.2f}%' #4.3% were bought only once","metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:23:13.661453Z","iopub.execute_input":"2022-05-30T06:23:13.661705Z","iopub.status.idle":"2022-05-30T06:23:13.667986Z","shell.execute_reply.started":"2022-05-30T06:23:13.661677Z","shell.execute_reply":"2022-05-30T06:23:13.667303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lastbuy = train.groupby('article_id').t_dat.max()\nday = np.timedelta64(1,'D')\nmaxd = train.t_dat.max()\nfor i in range(30, 720, 30):\n    print(f'{(lastbuy>maxd-i*day).mean()*100:.2f}% bought last time {i*day} ago ')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:21:09.007263Z","iopub.execute_input":"2022-05-30T06:21:09.008071Z","iopub.status.idle":"2022-05-30T06:21:09.081323Z","shell.execute_reply.started":"2022-05-30T06:21:09.008032Z","shell.execute_reply":"2022-05-30T06:21:09.080625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#about 80% no longer break than 90 days so we take last 90 days (only 40% items)\n#check for same for customer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cbreak = train.groupby('customer_id').t_dat.apply(mbreak)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:33:34.441103Z","iopub.execute_input":"2022-05-19T11:33:34.441429Z","iopub.status.idle":"2022-05-19T12:04:48.676699Z","shell.execute_reply.started":"2022-05-19T11:33:34.441397Z","shell.execute_reply":"2022-05-19T12:04:48.674604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmacs = toytrain.groupby('customer_id').t_dat.max()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = np.dat(1,'D')\ndmax = train.t_dat.max()\nfor i in range(30, 720, 30):\n    print(f'{(cmacs>dmax-i*d).mean()}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cbreak.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T11:31:19.044797Z","iopub.execute_input":"2022-05-19T11:31:19.045079Z","iopub.status.idle":"2022-05-19T11:31:19.064333Z","shell.execute_reply.started":"2022-05-19T11:31:19.045049Z","shell.execute_reply":"2022-05-19T11:31:19.063396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking how much can cut from customer and articles using breaks and last buy data\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ceck % of customer buying more same item more than one time based on the table above (nbuys >1)\nnumbuys = train.groupby(['customer_id', 'article_id']).t_dat.nunique().reset_index().rename(columns = {'t_dat':'nbuys'})\ncust_max_buys = numbuys.groupby('customer_id').nbuys.max()\ncust_max_buys.value_counts(normalize=True)*100 #72% buys dont buy same item in different date\n\n#checking pattern for the multibuyrsn (leave NA for better normalize ) \nmultibuys = numbuys.groupby(['customer_id', 'nbuys']).size().to_pandas().unstack()#fill_value=0)\npd.set_option('display.float_format', '{:.4f}'.format)\nmultibuys[2].value_counts(normalize=True) #46% one case, 20% two case, 10% 3 case","metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:10:17.692775Z","iopub.execute_input":"2022-05-30T06:10:17.693239Z","iopub.status.idle":"2022-05-30T06:10:19.556408Z","shell.execute_reply.started":"2022-05-30T06:10:17.693203Z","shell.execute_reply":"2022-05-30T06:10:19.555593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ceck % of customer buying more same item more than one time based on the table above (nbuys >1)\nnumbuys = train.groupby(['customer_id', 'article_id']).t_dat.nunique().reset_index().rename(columns = {'t_dat':'nbuys'})\ncust_max_buys = numbuys.groupby('customer_id').nbuys.max()\ncust_max_buys.value_counts(normalize=True) #72% buys dont buy same item in different date","metadata":{"execution":{"iopub.status.busy":"2022-06-01T03:36:23.207534Z","iopub.execute_input":"2022-06-01T03:36:23.207769Z","iopub.status.idle":"2022-06-01T03:36:23.630865Z","shell.execute_reply.started":"2022-06-01T03:36:23.207735Z","shell.execute_reply":"2022-06-01T03:36:23.630193Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cancel this two, need functio to diplay images nucely and other to display stats\n#show pic and sales data for specific encoded article\ndef artidata(aid, df, stats=False, path=path):\n    faid = '0' + str(aid)\n    fp = path/'images'/faid[:3]/(faid+'.jpg')\n    if fp.exists():\n        im = Image.open(fp)\n        im.thumbnail((128,128))\n        display(im)\n        im.close()\n        print(aid)\n    if stats:\n        tf = df.loc[df.article_id==aid]\n        tf = tf.to_pandas()\n        sns.countplot(\n            x=\"t_dat\", data=tf, \n            order = tf.t_dat.sort_values().values\n            )\n        _ =plt.xticks(rotation=90)\n        plt.show()\n        print(aid*2)\n        \n#show  data for specific encoded customer\ndef custidata(aid, df, path=path):\n    tf = df.loc[df.customer_id==aid] \n    sns.countplot(\n        x=\"t_dat\", data=tf.to_pandas(), \n        order = tf.to_pandas().t_dat.sort_values().values)\n    _ =plt.xticks(rotation=90)\n    plt.show()\n    print(aid)     \n    \n#another reason to cancel this function \ntt = train.groupby('article_id').sales_channel_id.count().sort_values().tail()\nttl = tt.index.values_host\nfor i in ttl:\n    artidata(i, train, stats=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T05:25:53.12966Z","iopub.execute_input":"2022-06-08T05:25:53.129952Z","iopub.status.idle":"2022-06-08T05:25:53.141693Z","shell.execute_reply.started":"2022-06-08T05:25:53.129906Z","shell.execute_reply":"2022-06-08T05:25:53.140893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tt = toytrain.groupby('customer_id').sales_channel_id.count().sort_values().tail()\nttl = tt.index.values_host\nfor i in ttl:\n    custidata(i, toytrain)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T09:31:34.429302Z","iopub.execute_input":"2022-05-23T09:31:34.429878Z","iopub.status.idle":"2022-05-23T09:31:35.256191Z","shell.execute_reply.started":"2022-05-23T09:31:34.429841Z","shell.execute_reply":"2022-05-23T09:31:35.255518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pure cold start - customer with no buy history\n#index is the maping so just need index numbers no no in train\ncold_start_idx = submit.loc[~submit.index.isin(train.customer_id.values)].index\n#9699 cusomers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling","metadata":{}},{"cell_type":"markdown","source":"Taken from this notebook: https://www.kaggle.com/code/titericz/h-m-ensembling-how-to","metadata":{}},{"cell_type":"code","source":"#need to prepare first a DF with all the predictions we want to ensemble and after that adjust the:\n#W variable based on the number of models to enemble\n#REC list appending all modles \ndef cust_blend(dt, W = [1,1,1]):\n    \n    #Create a list of all model predictions\n    REC = []\n    REC.append(dt['prediction0'].split())\n    REC.append(dt['prediction1'].split())\n    REC.append(dt['prediction2'].split())\n    \n    #Create a dictionary of items recommended. \n    #Assign a weight according the order of appearance and multiply by global weights\n    res = {}\n    for M in range(len(REC)):\n        for n, v in enumerate(REC[M]):\n            if v in res:\n                res[v] += (W[M]/(n+1))\n            else:\n                res[v] = (W[M]/(n+1))\n    \n    # Sort dictionary by item weights\n    res = list(dict(sorted(res.items(), key=lambda item: -item[1])).keys())\n    \n    # Return the top 12 itens only\n    return ' '.join(res[:12]) #RK = return in for each row, all the avove is done per Series\n\nsub0['prediction'] = sub0.apply(cust_blend, W = [1.05,1.00,0.95], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Write 5/7 tricks learned on DEA from GM notebook\n#Summarize all soultion learned by now \n#Explore ML/DL solution (the one LGBM that sort candidtes???)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cdf = cudf.DataFrame({'c1':['a', 'a', 'b', 'c', 'e', 'f', 'g'], 'c2':[10,  10, 20, 30, 40, 50, 60], 's1':[2, 1,1,1,1,1,1]})\ncdf2 = cudf.DataFrame({'c1':['a', 'b', 'c', 'e', 'f', 'g'], 'c2':[10, 2, 30, 4, 50, 6], 's2':[1, 1, 1, 1, 1, 1]})\ncdf3 = cudf.DataFrame({'c1':['a', 'b', 'c', 'e', 'f', 'g'], 'c2':[1, 20, 30, 4, 50, 60], 's3':[1,1,1,1,1,1]})\ncdlist = [cdf, cdf2, cdf3]","metadata":{"execution":{"iopub.status.busy":"2023-01-13T04:49:49.709131Z","iopub.execute_input":"2023-01-13T04:49:49.709586Z","iopub.status.idle":"2023-01-13T04:49:49.754262Z","shell.execute_reply.started":"2023-01-13T04:49:49.709541Z","shell.execute_reply":"2023-01-13T04:49:49.753319Z"},"trusted":true},"execution_count":null,"outputs":[]}]}